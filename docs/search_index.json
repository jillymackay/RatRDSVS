[["index.html", "R @ R(D)SVS R @ R(D)SVS", " R @ R(D)SVS Jill R D MacKay 2023-09-13 R @ R(D)SVS This book will help you get started in R. It’s designed for students (and staff) at R(D)SVS, and is free to use and adapt. "],["about.html", "About this book 0.1 How to use this book 0.2 Tips for using an online textbook 0.3 Licensing", " About this book This version of the book was last published on 2023-09-13 0.1 How to use this book This book is a manual to help you get started in R. There is text like this which is important. At the start of every chapter it will summarise what you need to know, or let you know if you can skip the chapter. You will also see text like this which is called a code block print(&quot;Hello, I am a code block!&quot;) Throughout the book, code blocks will be used to give you examples of how R works. Sometimes code blocks will also have the output block following it. ## [1] &quot;Hello, I am an output block!&quot; You can tell output blocks because they have two ## symbols to show you that it is output. Often we will see code blocks and output blocks together, like this: print(&quot;Hello, I am a code block with an output block!&quot;) ## [1] &quot;Hello, I am a code block with an output block!&quot; Sometimes you will see text like this - which is really important: When you are running code blocks in R make sure to only copy and paste the code block - not the output block! Pasting an output block into R will not do anything. You will also sometimes see bits of code or words in text like this. This is usually the name of something in R, or a short piece of code that I didn’t want to put in a full code chunk. You will also see text like this, which will explain in plain English what the r code is doing. E.g. the code 2 + 2 tells R to perform the following calculation: 2 + 2 Sometimes there will also be links to videos. These are optional for those learners who like to watch something being demonstrated. Finally - sometimes there will be footnotes!1 0.1.1 As a student in class If I have set this book as your reading, you will be told what chapters you need to read and when in class. You are very lucky because you have the person who wrote the book being responsible for your teaching - and you can ask me questions at any point, preferably in our discussion boards. Although here’s something you might like to know - I really don’t mind if you read ahead. If you start working and find you love this subject, keep on going with it. And something else - you are also free to google other ways of learning R. In fact we have a whole chapter about how to find answers online and where else you can go. 0.1.2 As a student learning independently If you are using this to teach yourself R you are probably a postgrad student. I would expect you to look at the chapter headings and skip around to the sections that are useful. My big tip for you would be to use the Exercises to test yourself throughout. 0.1.3 As a teacher Please feel free to mix/match and adapt these teaching materials in any way you see fit! Credit not necessary (but appreciated!) If you have any questions you are best talking to me on Mastodon jilly_mackay@fosstodon.org, Bluesky jillymackay.bsky.social or best of all - raise an issue with the book on github and get acknowledged for it! 0.2 Tips for using an online textbook An online textbook like this one can be a lot more flexible than a paper one. You can download it and print it if you want to make notes in the margins - the book is free for you to do what you want with it. You can change the colours on screen by hovering your mouse over the ‘A’ icon at the top left hand side of the page. 0.3 Licensing These materials are licensed under a CC0-1.0 License. Find out more here You can also find the materials here at the associated github repository. Footnotes might contain something useful - or something silly. I guess you’ll need to click it to see! But remember, you can’t trust me implicitly - or anyone really↩︎ "],["install.html", "Chapter 1 Installing R 1.1 Install R 1.2 Install R Studio 1.3 Using the Cloud 1.4 Install R and R Studio - Video Instruction", " Chapter 1 Installing R You can skip this chapter if you already have access to R and R Studio, and feel confident navigating between the console and environment panes. To use R on your own PC or laptop, you will need to install two things: R (the programming language) R Studio (a software that makes R easier to run) You can also access R Studio online which can be great if you have a good internet connection, or are on a computer that you don’t have admin rights to. R Studio is an integrated development environment which makes it easier to use R. It is made by a company called Posit who create a lot of statistical coding related projects and have a really cool and supportive community. Important: If you want to use R and R Studio on your own device you need to install both. If you prefer watching instructions you can watch the installation videos below. 1.1 Install R R is the programming language we will use. It is freely available, meaning you do not need to pay for R. To install R, you will need to navigate to (or click the link to) the Comprehensive R Archive Network - cran. While there, you want to download R for your operating system (most likely Windows or Mac). Click the link that names your operating system and look for the instructions that say install R for the first time. There will be a link that says something like Download R 4.3.1 for Windows. (The exact version number is not really important yet). After you have installed R, you can check to see if it works, or go straight to installing R Studio 1.2 Install R Studio R Studio is a tool that helps make R more user friendly. It also lets you collect all your data files into ‘projects’. This is very useful for managing your projects and ‘workflows’ (more on that here). To download R Studio follow this link. You want the R Studio Desktop Free version. You will not need to pay for anything to use R as part of your studies at R(D)SVS. When you click the download link above, you will find a button at the bottom of the page to download R Studio Desktop. After you click that button you will be able to choose the version that works for you. Follow the instructions on the page for your operating system. 1.2.1 Note for those using a managed PC If you are using a university managed PC then the Software Centre will take care of this for you. All open access university PCs should have a version of R Studio already installed. 1.2.2 Quick R Studio Navigation Guide If you have never coded before, R Studio might look very intimidating. In this short section we will walk through the different parts of R Studio. If you find yourself getting lost later in the book, come back here to get a refresher. In Figure 1.1 you will see a view of R Studio as you first open it. There may be some differences, e.g. you will likely have a different R Version number. Figure: 1.1: R Studio Overview 1.2.2.1 Click on File &gt; New File&gt; R Script Now your screen should look like Figure 1.2. You will have four panes in your window, and each pane can be dragged around or minimised. Note - you want to create a new R Script file, not an R Markdown or R Notebook file. We will do that next. Figure: 1.2: R Studio Overview with Script File 1.2.2.2 The file pane On the top left hand side of the R Studio window you can view the untitled R Script file you have just opened (Figure 1.3). Here you will see views of your files (mostly R Script files and R Markdown files in this book), and views of your data. Figure: 1.3: The active file pane 1.2.2.3 The console If you were using plain R (without R Studio) this console window is all you would see. Where you see this &gt; symbol in the console you can type in a simple maths equation. I recommend you type something now, perhaps 2 + 2 When you have finished typing, hit enter on your keyboard and see what the answer is. Type 2 + and then hit enter. What happens? You will need to hit ‘escape’ (‘esc’) on your keyboard to make the &gt; symbol appear again. Hitting ‘enter’ asks R to perform the last command. Because we didn’t tell R what it had to add, it kept waiting to find out what would come next. Figure: 1.4: The console pane 1.2.2.4 The Environment pane The Environment is shown in Figure 1.5. At the moment, your environment will be empty. We will talk more about what goes here in the environment section of getting started. Figure: 1.5: The environment pane 1.2.2.5 Files, plots, packages, help and viewer The final pane, shown in Figure 1.6, has five tabs in it by default. These are: Files Think of this like a Windows Explorer window (folder window) on your PC. It will show you all the files in your current folder and may just show your .Rproj file right now. Plots When we start drawing charts they’ll be saved here. Packages Other wonderful people in the R Community write lots of clever code that can be ‘packaged’ up so we can use it. Instead of writing out all the code you need to make a chart, you can use someone’s package instead. We will learn more about these in packages Help If you ever get stuck with a commaned you can type ?command_name to view the command’s documentation. Try it now by type ?summary in the console window. Remember you need to press enter on your keyboard after typing the command. Viewer This tab will show you more complicated things, like if you use R to build an html page (or a book like this one!) Ignore it for now. Figure: 1.6: File, plot, package and help viewer 1.2.2.6 Customisation Data analysis is fun, and its also personal. You can customise your R Studio to look the way you want it to. Go to Tools &gt; Global Options (Figure 1.7) Figure: 1.7: Global options And then go to Appearance &gt; Editor Theme to play around and find the colour scheme, font, and R Studio Theme you like best (Figure 1.8) Figure: 1.8: Global options, themes Personally I like the theme XCode, but it’s your R Studio! You choose the one you like. 1.3 Using the Cloud If you would like to use Posit Cloud (formerly R Studio Cloud), you will need to create an Posit account. This can be a whole new account, a google account, a Clever account, or a github account. I would recommend you get a github account (you can jump to read why here). After you have logged in, you can access a workspace. A workspace is like an R Studio Project. You should use the Cloud Free Plan. Again, we do not expect you to buy anything to use R or R Studio. You will have a maximum of 15 projects you can use, and less storage space than using your own device. 1.4 Install R and R Studio - Video Instruction If you prefer to get your resources in video format, there’s an explanation of installing R and R Studio here and using R Studio Cloud here. Please note, these videos are a bit older now so you should expect to see some minor differences, e.g. in version numbers. "],["start.html", "Chapter 2 Getting Started 2.1 The Big Secret 2.2 Projects 2.3 Environment 2.4 Packages 2.5 R Markdown vs R Scripts 2.6 Video Introductions", " Chapter 2 Getting Started You can skip this chapter if you can: Use the console to input simple lines of code Install and load a package Recognise a function To use this chapter you will need a working version of R and R Studio. If you can open R Studio and type 2 + 2 in the console you are good to go. If that is confusing, you might need to jump back to installing R Studio and the section navigating R Studio. 2.1 The Big Secret Okay. This is the Big Secret. The thing you will not believe. The most important thing you’ll learn about R. You should copy and paste other peoples’ code. Yes, I’m serious. You should copy and paste and edit code you find online2. In this book you should always be copying bits of code and pasting it into your console. That is why I publish the code alongside each question and demonstration. It took me about ten years of working in R to get over my fear of copying and pasting other peoples’ code. Now I even have a repository of all the code I use over and over so I can copy and paste my own code. Most people will not remember code off the top of their head. This is why we use the help function (try typing ?summary to see what it says) to look at the documentation, and resources like Stack Exchange and Google to help us (more in troubleshooting). 2.1.1 What about AI like ChatGPT? Large Language Models like ChatGPT and Google Bard can also be applied to coding problems. For example, you could ask ChatGPT how to rearrange a dataset. This can be really helpful, but as with all things AI related, you need to be very cautious and test the answer to make sure it’s right. Often it doesn’t choose the smartest way. The use of Large Language Models in coding is a subject of debate. Stackoverflow (a big community resource) currently does not allow any ChatGPT answers/questions on its site. At present, Edinburgh has a policy for the use of ChatGPT and similar tools here. It boils down to: don’t claim it as your own work and be cautious of its reliability, accuracy, and the legality of your question. Whenever we talk about the use of Large Language Models I think its really important to remember that they boil down to fuzzy jpgs of the web some time ago. That’s a really good article by the way. 2.1.2 Exercise You might be very skeptical here about how much I want you to be copying code. If you’re not convinced, I suggest you go watch my RStats Crush the amazing David Robinson do one of his Tidy Tuesday screencasts. David Robinson has forgotten more R than I’ll ever learn, and see - he still copies and pastes code. 2.2 Projects I would like to begin by creating a new project in R Studio. Open R Studio Go to File &gt; New Project Set up a new project either in an existing directory (folder), or in a new one. Ignore repositories for now. You can call this project anything you like. I recommend something short like ‘start’. A R Project sits inside a folder. Everything inside that folder is part of that R Project. Let’s say you have a folder called: mydata and you create an R Project called analysis. The analysis.Rproj will live inside the mydata folder, and it will ‘see’ all the data, files, and any other folders, you have in mydata. Using R Studio you will get a lot better at ‘directory management’, or knowing where you have saved things. 2.3 Environment In your brand new project you will have a clean environment (refer back to Figure 1.5). Our first exercise is going to explore what the environment really means. 2.3.1 Exercise - Environment When we first installed R, you were typing mathematical equations into the console. Let’s remind ourselves of this by running an equation now. In the console, type a long and complicated equation, then hit ‘enter’ when you’re done. 500 + 23 / 91 ## [1] 500.2527 If this was a calculation we often ran, we might want to save the answer. We can do this by using the assign symbol &lt;-. For example, if we type this: x &lt;- 500 + 23 / 91 … you will see that we simply get a new line returned. We don’t get the answer. However, in your environment you will see under the heading ‘Values’, a new thing called x. Enter the following code: x ## [1] 500.2527 R remembers what x is and recalled it. This is useful if we want to do more with x after we’ve calculated it. x /100 ## [1] 5.002527 As well as values, we can ask R to remember a string of text as well. Try this: hello &lt;- &quot;Computers always print &#39;Hello World!&#39; but we never say hello back&quot; print(hello) ## [1] &quot;Computers always print &#39;Hello World!&#39; but we never say hello back&quot; But R is very fussy. What happens if you run this code: print(HELLO) You see, R is case sensitive, meaning we have to be careful to always type things the same. This is one of the reasons why R Studio is really handy, because you can see what you’ve saved in your environment. Now type this: hello &lt;- &quot;Hello, R!&quot; Take a guess what is going to happen before you type the next line. Were you right? print(hello) ## [1] &quot;Hello, R!&quot; 2.3.2 Exercise - Functions 2.3.2.1 The Basic Function A function is a handy way to bundle together some lines of code. David Robinson says that if you run a few lines more than twice you should write a function to do it instead. I’m going to contradict myself now. Functions are really important in R, but I’m not going to ask you to write one anywhere else but in this exercise. Most things in this book I expect you to practice over and over, but functions are useful to understand packages, and that’s why we’re going to talk about them now. Let’s create a function to make R welcome us. Copy and paste this code into a new script file (you may want to save it as your functions example), and then run it. mynameis &lt;- function (name) { print (paste0(&quot;Hello, &quot;, name, &quot;, how are you today?&quot;)) } mynameis (&quot;Jill&quot;) ## [1] &quot;Hello, Jill, how are you today?&quot; What is this code saying? Create a new thing called mynameis Give (&lt;-) mynameis the function purpose. Everything inside the curly brackets {} is the function. The function should make a new thing called name. The function should print the result of pasting a string (paste0) of things, the words Hello,, whatever you said name should be, and , how are you today?. You may have noticed that, just like when we were assigning x a value, we now have something in our environment called mynameis. Whenever we type mynameis(\"Jill\") into the console, or execute that line in a script file, R knows it should look to the environment and run the code that’s been bundled up into that package. For example, we could make R welcome us in exactly the same way by typing: print (paste0(&quot;Hello, &quot;, &quot;Jill&quot;, &quot;, how are you today?&quot;)) ## [1] &quot;Hello, Jill, how are you today?&quot; But we would need to change the name every time we wanted welcome a new user. We are lazy, so we use the function to reduce the amount of things we need to change to get a different outcome. Functions make our code more standardised. If we all define the mynameis function at the start of our documentation, we don’t need to worry about accidentally deleting an important character in the code. This is especially important as we like to copy-paste code when we need to. Most scientists will probably not write their own functions, but you should know how they work. 2.4 Packages If people write very good and useful functions, they usually want to share them. They can do this with packages. There are packages for everything, packages for drawing maps, packages for analysing data, packages for suggesting how to analyse your data, packages to give you more data, packages for tidying your data . . . this is one of the reasons we might all have different bits of code. There can be lots of ways to do something. 2.4.1 Exercise - Install and Load a Package Packages work in two stages. The very first time you need a package you need to install it. Then, every time you start R you need to load the package into your library. First you install. Next you load into library. To install a package you need to type install.packages(\"package_name\"). To load it into your library you need to type library(package_name). When you load a package from your library it goes into a special hidden environment where you can access all the functions that the package has written for you. For example, we will make great use of the tidyverse package in this book. You need to start by installing it with: install.packages(&quot;tidyverse&quot;) While the package is downloading and installing you may see a little red ‘stop’ sign in the top right hand corner of your console window. This means R is working, and you shouldn’t add any new commands until it has finished and the new line symbol appears again in the console (&gt;). When it has finished downloading &amp; installing, you need to type: library(tidyverse) to load it. You’ll see that nothing has appeared in your Environment, but actually you have a whole host of new functions &amp; data you can play with. For example: slice(starwars) Which should give you this: ## # A tibble: 0 × 14 ## # ℹ 14 variables: name &lt;chr&gt;, height &lt;int&gt;, mass &lt;dbl&gt;, hair_color &lt;chr&gt;, skin_color &lt;chr&gt;, eye_color &lt;chr&gt;, birth_year &lt;dbl&gt;, sex &lt;chr&gt;, ## # gender &lt;chr&gt;, homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; Now we have a new dataset (starwars) and a new thing we can do (slice the top of the data). We will talk much more about tidyverse in the data chapter. You will often get warnings in R when you type these code blocks. To keep the book tidy, I have suppressed (hidden) the warnings in this book. Don’t worry about them - they’re usually just trying to keep packages compatible with older versions of R 2.5 R Markdown vs R Scripts In R Studio you can use R Script files and R Markdown files to save your work. Both of these files can be created, edited, saved, and opened in your R Project file. You can send R Script and R Markdown files to other people, so they can edit and share your code too. If you are reading this book as part of coursework I’ve set you, you will likely have an assignment where you neeed to download and edit an R Markdown file, then send it back to me. 2.5.1 R Scripts R script files are great for testing out small chunks of code on their own. Go to File &gt; New File &gt; R Script or press Ctrl + Shift + N to open one. Everything you type in an R script file is executable code. Unlike working in the console, you can type multiple lines of code in an R script, and run each one by hitting ctrl + enter. You can’t type plain text in an R Script file. Text has to be annotated by the comment symbol (#) # In an R script file - you can write comments # Which always begin with a hashtag # Comments can be useful to say things like what date you wrote the script on, # or what you were trying to do. print(&quot;You can also use script files to write multiple lines of code&quot;) print(&quot;and edit those lines of code&quot;) # before you run them by pressing ctrl + enter at the same time # when your cursor is on that line. 2.5.2 R Markdown R Markdown allows you to combine code with actual text. This book was written in R Markdown In R Markdown you can combine normal text with code blocks, and R knows to only treat the text inside the code blocks as code. Figure: 2.1: An example of R Markdown with text and code There is a great R Markdown cheatsheet from R Studio here. I had this printed out and hung above my desk for years. 2.5.3 A video on R Markdown If you would like to watch me using R Markdown, you’re in luck! Theres a video here. 2.6 Video Introductions There is a short introduction to R and R Studio in Video format here: Video 1 Video 2 If you’re reading this as part of your coursework you might be panicking about plagiarism here, after all, we spend a lot of time telling you plagiarism is the worst thing you could ever do and that we’ll use software to detect it. Code is a bit different. We are always trying to find the most efficient way of doing something, and so ideally you would all write code that was identical. Sadly, humans naturally differ in the way the think about problems. My job would be a lot easier if everybody thought the same. If I have set you this book as reading, I can swear to you I will never put your code through a plagiarism checker. That would be very stupid.↩︎ "],["data.html", "Chapter 3 Data 3.1 Packages for this chapter 3.2 Built In Data 3.3 Exercise 3.4 Data you create 3.5 Exercise - Creating data 3.6 Loading data from your hard drive 3.7 Loading data 3.8 Exercise 3.9 Exercise", " Chapter 3 Data You can skip this chapter if you: Are comfortable working with dataframes and tibbles Know about and are comfortable using some of R’s inbuilt data packages Are comfortable loading .csv and .xlsx files into R 3.1 Packages for this chapter In this chapter you’ll want the following packages loaded in your R session. At the start, you will need to run the following code. Remember if you don’t have a package you can install it with the install.package(\"package_name\") command. library(tidyverse) library(readxl) 3.2 Built In Data Many packages, such as the tidyverse package, and the default 3 datasets package come with data in them. These datasets can be really useful for testing code that you’re unfamiliar with, because you know what the data should look like, and how it should perform. Some common datasets are: mpg Fuel economy data from 1999 to 2008 for 38 popular models of cars ChickWeight Weight versus age of chicks on different diets starwars Name, height, weight, age, and other characteristics of Star Wars characters iris The measurements (in cm) of the sepal length and width and petal length and width of 50 flowers in 3 species of iris There are many, many more. You can explore some of them by typing datasets:: into the console and instead of pressing ‘enter’, scroll the menu that pops up to help you autocomplete your command. You might wonder, what’s the :: in the datasets:: command. The :: is very useful and tells R to look inside the datasets package without having to actually load it into your library which can be very quick and easy. But be careful! If you are sharing your code with lots of other people, this can sometimes make it a bit harder for other people to run your code. This is called ‘environment management’ and something you’ll learn more about as your R journey continues. 3.3 Exercise For our first exercise we are going to look at some of the inbuilt datasets. What happens when you enter the following code4 into the console? View(mpg) You should see something like Figure 3.1 Figure: 3.1: The result of the ‘View(mpg)’ command The View() command is very handy for getting a quick window onto what data looks like. Unfortunately, the View() command only really works for you getting a look at your data. If you wanted to share your code, you would be better using a command which prints some of the data into the console. So - what functions will tell R to print data into the console? You can view the top of a dataset with head() head(mpg) ## # A tibble: 6 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compact ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compact ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact Note that this code has produced an output block. Unlike View() where I had to share a screengrab to show you what the output of the code was. This is why head() is a lot more reproducible than View() and should be your default for code sharing. See more about this in workflows. Unsurprisingly we can also view the bottom of a dataset with … tail(mpg) ## # A tibble: 6 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 volkswagen passat 1.8 1999 4 auto(l5) f 18 29 p midsize ## 2 volkswagen passat 2 2008 4 auto(s6) f 19 28 p midsize ## 3 volkswagen passat 2 2008 4 manual(m6) f 21 29 p midsize ## 4 volkswagen passat 2.8 1999 6 auto(l5) f 16 26 p midsize ## 5 volkswagen passat 2.8 1999 6 manual(m5) f 18 26 p midsize ## 6 volkswagen passat 3.6 2008 6 auto(s6) f 17 26 p midsize Another very useful way to look at a dataset is with the summary() function, which takes its best guess at how to summarise each variable in the dataset. summary(mpg) ## manufacturer model displ year cyl trans drv ## Length:234 Length:234 Min. :1.600 Min. :1999 Min. :4.000 Length:234 Length:234 ## Class :character Class :character 1st Qu.:2.400 1st Qu.:1999 1st Qu.:4.000 Class :character Class :character ## Mode :character Mode :character Median :3.300 Median :2004 Median :6.000 Mode :character Mode :character ## Mean :3.472 Mean :2004 Mean :5.889 ## 3rd Qu.:4.600 3rd Qu.:2008 3rd Qu.:8.000 ## Max. :7.000 Max. :2008 Max. :8.000 ## cty hwy fl class ## Min. : 9.00 Min. :12.00 Length:234 Length:234 ## 1st Qu.:14.00 1st Qu.:18.00 Class :character Class :character ## Median :17.00 Median :24.00 Mode :character Mode :character ## Mean :16.86 Mean :23.44 ## 3rd Qu.:19.00 3rd Qu.:27.00 ## Max. :35.00 Max. :44.00 The summary() function highlights an important aspect of R - it knows the difference between numbers and characters. In the output block we can see that manufacturer is given the Class character. R knows it can’t create a mean and a median from this data, so it simply tells you the Length of that data instead. What do you think the 234 refers to here? Answer in the footnote5. 3.4 Data you create Sometimes you will want to test something on pretend data, or run something on a very small dataset. In these cases, we can create a dataset in our environment. This can be really useful for troubleshooting because you can create data that shows your problem, and this is easier to share with others. For example, lets say we we were interested in this idea that R only sees manufacturer in mpg as a string of letters. How can we tell R that manufacturer is in fact a category? We can test on a smaller, simpler dataset. 3.5 Exercise - Creating data We are going to use a new function here called tibble which will make a dataframe quickly and easily. Before when we were typing things like x &lt;- 1, we created a single thing in the environment. Now we want to create multiple thing (rows and columns) and that’s what the tibble function will do. Try the following and watch what happens in your environment (you’ll need the tidyverse loaded - see above) dat &lt;- tibble(x = c(1, 2, 3, 4, 5, 6), y = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;)) You can View(dat) to look at this (or use head(dat)). Let’s try and replicate our weird summary(mpg) character issue. summary(dat) ## x y ## Min. :1.00 Length:6 ## 1st Qu.:2.25 Class :character ## Median :3.50 Mode :character ## Mean :3.50 ## 3rd Qu.:4.75 ## Max. :6.00 We can replicate the issue. You can see that the y variable has the class: Character. What can we do about it? We essentially want to tell R that y is not a character, but a factor6. We can ask R more directly what y is by telling it exactly where to look. is.character(dat$y) ## [1] TRUE The $ symbol tells R that it needs to look inside dat for y. Try typing out is.character(y) or is.character(dat::y) and see what happens. What are those error messages telling you? 7 If the is.character() function asks R if the named object is a character, how do you think we ask R if the named object is a factor? Try typing out your answer in the console to see what happens. We therefore need to ask R to change the data. We can do this like so: as.factor(dat$y) ## [1] A B A B A B ## Levels: A B And we get a response. But what happens if we run summary(dat) again? summary(dat) ## x y ## Min. :1.00 Length:6 ## 1st Qu.:2.25 Class :character ## Median :3.50 Mode :character ## Mean :3.50 ## 3rd Qu.:4.75 ## Max. :6.00 Why do you think this happens? You should take time to stop and answer these questions before reading on - practice and thinking about R is the best way to learn. It happens because we haven’t actually changed the data. With as.factor(dat$y), R told us the answer, but didn’t edit the object in any way. To change the dat dataframe, we need to use the assign (&lt;-) function to make sure R saves it into the environment. Here’s the fun part. We can do this in two (or actually - many) different ways. In a lot of ways, its personal preference which one you choose . . . To demonstrate, we will make two new dataframes. Option 1 option1 &lt;- tibble(x = dat$x, y = as.factor(dat$y)) Option 2 option2 &lt;- dat |&gt; mutate(y = as.factor(y)) In Option 1, we are essentially saying something like: Make a new thing called option1 That thing is a tibble That tibble should contain x, from dat (dat$x) That tibble should contain y, from dat, made into a factor (as.factor(dat$y)) Whereas in Option 2, we are saying: Make an new thing caled option2 That thing should start the same as dat We use the pipe command (|&gt;) to say ‘and then’ So take dat and then change something in dat (the mutate function) Make y a factor (y = as.factor(y)) With Option 2, we don’t need to specify dat$y because we are using the pipe function (|&gt;) to tell R we are still working with the dat dataframe. The |&gt; pipe is a relatively recent implementation, but tidyverse has utilised the %&gt;% pipe from the magrittr package for years. Lots of documentation will use the magrittr pipe %&gt;% online, and even in this book. Either is fine and should both work in any cases. (There are a few special cases where they function differently which Hadley Wickham talks about here) Personally, I found the pipe function in tidyverse to be revolutionary when I learned it, and its by far my favourite way to write code. I find the code in Option 2 a lot easier to read than the code in Option 1, and we’ll be using tidyverse in the rest of this book. However, here’s another important thing: You do not have to code like I do - so long as I can run your code, we don’t have to approach a problem in the same way. This will become clearer as we work through the book. To test that we have done what we wanted to do - let’s run the summary() function again. summary(option1) ## x y ## Min. :1.00 A:3 ## 1st Qu.:2.25 B:3 ## Median :3.50 ## Mean :3.50 ## 3rd Qu.:4.75 ## Max. :6.00 summary(option2) ## x y ## Min. :1.00 A:3 ## 1st Qu.:2.25 B:3 ## Median :3.50 ## Mean :3.50 ## 3rd Qu.:4.75 ## Max. :6.00 And now we see that R has changed y into a factor, both ways, and now we have a different output to the summary() function. Thinking back to our mpg example - can you change the mpg dataset so that manufacturer is a factor? Can you change multiple variables to factors? Is tidyverse or base R easier to use for changing multiple factors? If you google, do you find other ways of changing mpg? Once you have tried these - you can view some of my answers here 3.6 Loading data from your hard drive Finally, sometimes you will have data delivered to you on a file that you will want to load in to R. These will usually be *.csv or *.xlsx files These can easily be read in with a function. The most commonly used function for *.csv files is read.csv. For excel files its read_excel from the readxl package. There are also ways to load data from URLs, and word documents, and all sorts! The skill most needed to load files from your hard drive is the ability to navigate and understand folder structures (sometimes called working directories). If your desktop has a hundred files then you should go and watch this video. We will use two different files for the rest of this chapter. You should download the two files and save them into the folder you are using for your R Project. We will explore directories a bit in the next section, but its better to start good practice now. The two files are: ch2_planets.xlsx ch2_planets.csv You will find them on your Learn course under this week’s materials. If you are learning independently, you will also find them in the data folder on my github page. 3.7 Loading data 3.7.1 Loading from a CSV There are two main ways to load a .csv file. The easy way is to navigate to the environment tab and click on Import Dataset Scroll down to From Text (base) and click through the import wizard, navigating to where you saved your file. Here is a short demonstration: Figure: 3.2: Using the Import Dataset wizard to load a csv file Note that the the import wizard runs code in the console which says something like: ch2_planets &lt;- read.csv(&quot;~/data/ch2_planets.csv&quot;) If you copy this code you can run it yourself. You know by now that anything to the left of the assign command (&lt;-) is the name of the thing you’re creating. 3.8 Exercise Change this code so you’re creating a dataset called dat not ch2_planets. Answer here 3.8.1 Loading from an Excel file Similar to above, you can upload an .xlsx file in two different ways, using the wizard, or by typing in the code. To use the wizard, you again navigate to the environment tab and click on Import Dataset Scroll down to From Excel and click through the import wizard, navigating to where you saved your file. Depending on what your excel file looks like Here is a short demonstration: Figure: 3.3: Using the Import Dataset wizard to load a xlsx file This time, note that the console first loads a package (library(readxl)) before it loads the data. library(readxl) ch2_planets &lt;- read_excel(&quot;ch2_planets.xlsx&quot;) Again you can copy this code and run the code itself in an R Script file, in a R Markdown file, or in the console, to load the data without going through the wizard. Why would you want to use code instead of the wizard? We will come to talk about repeatable workflows in a later chapter, but basically - if you send me an R Script file with the code for loading in the data, I can run that file without having to edit it. You can send me your script and your data file together, and I can work directly on it, which saves a huge amount of time and makes collaborating easier. 3.9 Exercise This time I want you to load the ch2_planets.xlsx file into your environment. I assume that you have saved the ch2_planets.xlsx file to your working directory in a folder called data. Your task: I don’t want to have to load the readxl library every time I read in an excel file. Is there a short cut we can use? Remember to try this - play about with the code and talk to your classmates/friends before looking at the answer. Answer here ‘Default’ here means one you won’t need to install or load into the library↩︎ If its not working - are you sure you have spelled it with a capital View?↩︎ The Length of these variables is the number of rows in each one, which for this case is the same for each variable because this is a nice, tidy dataset↩︎ A factor is also called a categorical variable, or a grouping variable. If you’re not sure you know what a factor is, wiki is a good place to review↩︎ is.character(y) should give you an error message like Error: object 'y' is not found because y by itself does not exist in your environment. There’s a way around this by ‘attaching’ data to your environment, which is a bit old fashioned and can result in problems down the line with your workflow (because you won’t necessarily know if the person you’re working with has also attached the data), so I recommend against it. is.character(dat::y) should give you an error like Error in loadNamespace(name) : there is no package called 'dat'. Unsurprisingly, this is telling you that the :: sequence tells R to look inside a package for a thing called y, but that package doesn’t exist. Packages and data frames are different things.↩︎ "],["charts.html", "Chapter 4 Charts 4.1 Data Visualisation", " Chapter 4 Charts You can skip this chapter if . . . You are confident you know the difference between a bar chart, histogram, scatterplot and line chart. You are confident you can read and interpret charts. You know what bad charts look like 4.1 Data Visualisation Data visualisation is one of the most important skills a scientst can learn, and being able to identify good and bad data visualisation will make you the life and soul of every party, and incredibly popular at all times. In this chapter, we’re not going to look at any R code. Instead we’re going to think about what data visualisation means. 4.1.1 When charts go bad Lets take a look at Figure 4.1. Figure: 4.1: US State of Georgia, COVID-19 Deaths, Source: https://ftalphaville.ft.com/2020/05/18/1589795135000/When-axes-get-truly-evil/ There is an excellent article from the Financial Times about this chart. I think this chart is a great example of why data visualisation is important. We use charts to communicate large amounts of data quickly and easily. Often, in life, we are trying to communicate something important. We are often trying to persuade someone to do something, like give us more research money, or to make a change that we want to see. What message do you think Figure 4.1 is trying to communicate? If you look at the x axis (the one that goes across the graph), what order are the dates in? 4.1.2 Chart anatomy 4.1.2.1 The two-axes rule Most charts will have two axes: The x axis which goes across the chart The y axis which goes up the chart (upp-y/down-y was how I used to remember it). As in data science we are often interested in does x affect y, we usually put the explanatory variable on the x axis. We try to always have the thing that drives variation going across the chart, whereas the thing that responds to variation goes up and down the chart. (The response-y variable.) You should always think about your axes - and they should be clear to the reader from the beginning. No messing with the order (see again Figure 4.1). Unless you have a very good reason, I would always have two axes. That means if you’re trying to put in a second Y axis, you are breaking the two-axes rule, and should re-evaluate your chart. If you’re trying to make a pie-chart, you are breaking the two-axes rule, and should re-evaluate your chart. Follow the two-axes rule, and your life will be a lot easier. 4.1.2.2 Why I hate pie charts Pie charts are the worst visualisation in the world. Figure: 4.2: The worst kind of chart describing proportion of people with type of pet (fictional data) Let’s take Figure 4.2 as an example. What proportion of people have a dog versus a cat. I’ll wait for you to start puzzling through that question. It’s very hard to distinguish between a pie slice that’s 2/5ths versus a pie slice that’s 2/7ths. Many people put labels on their pie charts, but if I need to read the label to understand the difference, why not just put the text in a table? Instead a bar chart shows use the exact ranking of this data and we can see how many more people have cats versus birds. Figure: 4.3: A much better chart describing proportion of people with type of pet (fictional data) 4.1.2.3 Figure headings Figure headings should be a clear description of what is in the chart. 4.1.2.4 Legends Like figure headings, legends should be informative and clear. They will always describe a categorical variable, and sometimes their job will be done by the x axis. 4.1.2.5 Colours The use of colours in charts is a curious thing. Colour can be very useful in a chart, but also very distracting. There’s another brilliant FT article on the use of colour to indicate gender in charts and all the ensuing complications. (University of Edinburgh folk can log in to the FT for free). 4.1.2.5.1 Advanced R users If you have gotten really into R, or you really like pretty colours, I highly recommend checking out Emil Hvitfeldt’s well maintained repository listing all the R colour palette packages out there. Personally I really like nord8 and LaCroixColoR and NineteenEightyR. Much of my life is spent tweaking colours on charts. 4.1.3 Bar charts Bar charts are good for describing a continuous (numerical) variable by a categorical (grouping) variable. When you are describing a continuous variable by a categorical variable, you are usually describing the mean of that category, but it can also be the median, or other measures. For example: Figure: 4.4: Average height (cm) of Star Wars characters by gender They can also be flipped around, particularly if the axis text is hard to read in one particular direction: Figure: 4.5: Average height (cm) of Star Wars characters by species 4.1.4 Histograms A histogram shows the relative frequency of a continuous variable. For example we can see the most common height of Star Wars characters, with 20 characters, is around 180cm: Figure: 4.6: Histogram of height (cm) of Star Wars characters 4.1.5 Scatterplots Scatterplots are good for showing the relationship between two continuous variables. Figure: 4.7: Average height (cm) of Star Wars characters by weight (kg) But we can use other aspects, such as shape or colour, to add in a categorical variable: Figure: 4.8: Average height (cm) of Star Wars characters by weight (kg) and species 4.1.6 Boxplots Boxplots are another way to examine a continuous variable by a categorical variable, but they give us a lot more information than a bar plot does. A boxplot shows you: The median value (the thick bar in the middle) The first quartile (the lowest part of the bar) The third quartile (the highest part of the bar) A lower hinge (the bottom thin line) which roughly equates to 95% of the data will not be below this value. An upper hinge (the top thin line) which roughy equates to 95% of the data will not be above this value. Any outliers (the dots) which are observations which lie outside of 95% of the data9 Figure: 4.9: Average height (cm) of Star Wars characters by gender 4.1.7 Infographics In our increasingly connected world, we are seeing more and more infographics. As they’re less standardised, there can be more room for interpretation. For example; As an Indian woman, I can confirm that too much of my time is spent hiding behind a rock praying the terrifying gang of international giant ladies and their Latvian general don't find me pic.twitter.com/sy9NHW9oTK — Sabah Ibrahim (@reina_sabah) August 6, 2020 Infographics can be extremely powerful, particularly when trying to communicate on social media. Unfortunately, sometimes the design choices can make it harder to understand exactly what the analysis has done. Infographics can be as misleading as bad charts. In general, I would focus on chart visualisations over infographics, even for public engagement. When you are very confident with making clear and readable charts, then you can start to think about infographics. I like nord so much its the colour scheme for this book↩︎ The hinges are actually calculated based on the boxplot.stats function which is a little more complicated than 95% of the data - but if you ever need to worry about this you have gone beyond what this book can teach you.↩︎ "],["ggplot.html", "Chapter 5 ggplot2 5.1 Why ggplot? 5.2 Building a ggplot object (a scatterplot) 5.3 Building a boxplot 5.4 Facets 5.5 Bar charts 5.6 Exercise 5.7 Videos 5.8 Useful resources", " Chapter 5 ggplot2 You can skip this chapter if: * You are confident using the ggplot2 package You can use the geom_bar, geom_point and geom_histogram arguments You can customise a ggplot2 object 5.1 Why ggplot? There is a fierce debate in the land of R. Some people think basic R should be taught first. Others think that you should learn how to do more complicated things first. I think you should start to learn ggplot2. ggplot2 is a really great way to learn R, and particularly the tidyverse approach to coding. It also makes beautiful charts. All my charts in the last chapter (even the dreaded pie chart) were made in ggplot2. 5.2 Building a ggplot object (a scatterplot) First - open an R Script or RMD file and load the tidyverse package: library(tidyverse) Let’s choose the diamonds dataset (from ggplot2 - you can check this by running ?diamonds), and take a look. head(diamonds) ## # A tibble: 6 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 The diamonds dataset is really big and has lots of variables. This makes it good for learning ggplot2 because you can continue using the same example dataset even when we need lots of variables. However, sometimes your PC might be a bit slow in rendering some of these charts. Be patient with it - especially when you see the red ‘stop’ sign in the top right of the console window. We want to take the diamonds dataset and then (|&gt;) send it to ggplot. In ggplot we will use the aesthetics argument (aes) to tell R what to put on the x axis and the y axis. diamonds |&gt; ggplot(aes(x = carat, y = price)) What do you think this code will do? Try this code first! This code ends up giving us a blank chart. This seems strange until you figure out that ggplot works by layering elements of a chart on top of one another: diamonds |&gt; ggplot(aes(x = carat, y = price)) Figure: 5.1: A ggplot object with no geom layers We need to tell R to add a geom layer, and we do that by adding a +. You may be interested to know that the + symbol is a precursor to the |&gt; symbol. Both ggplot2 and tidyverse were mainly written by Hadley Wickham who has spoken about why ggplot won’t ever use the pipe symbol. In this case, we want to add a geom_point layer, so we write the following: diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() Figure: 5.2: A ggplot object with a geom_point layer, the price of diamonds by their carat Look at how quickly and easily that worked. With three lines of code, you created a chart of 50,000 datapoints. That must make you wonder what else you can do . . . 5.2.1 Changing an axis in ggplot2 Let’s change the x axis on this chart. At the moment, we have a ‘tick’ mark at every carat, but what if we want to have a ‘tick’ mark at every 0.5 carats? All we need is another line of code. diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 5, 0.5)) Figure: 5.3: A ggplot object with a geom_point layer, the price of diamonds by their carat, x axis changed The scale_x_continuous line reads: Take the last three lines of code and then (+) * Change the scale of the x axis (scale_x_) which is a continuous (scale_x_continuous) variable. Change the numbers along the axis (breaks =) to a sequence (seq) The sequence starts at 0, goes to 5, and the spaces between them should be 0.5 We can go further by changing the limits within thescale_x_ command … diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 5, 0.5), limits = c(0, 6)) Figure: 5.4: A ggplot object with a geom_point layer, the price of diamonds by their carat, x axis changed Now we’ve told R: Change the scale of x Set new limits on x (limits =) The limits are a vector of two numbers together (c()) Start at 0 and end at 6 However, you’ll notice that the numbers don’t go all the way to the end. Have you spotted our mistake? We need to change the seq command earlier in the argument… diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) Figure: 5.5: A ggplot object with a geom_point layer, the price of diamonds by their carat, x axis changed And we can change the y axis in much the same way: diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 5, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) Figure: 5.6: A ggplot object with a geom_point layer, the price of diamonds by their carat, x and y axes changed 5.2.2 Changing themes Themes are a very cool way to quickly change the look of and customise your charts. Just like everything else in ggplot, we just want to add another layer of code. diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) + theme_bw() Figure: 5.7: A ggplot object with a geom_point layer, the price of diamonds by their carat, black &amp; white theme There are lots of different themes in ggplot. If you run that code again you can change theme_bw() to any of the following: theme_classic() theme_grey() theme_light() theme_linedraw() theme_minimal() theme_void() Which one do you prefer? Personally, I like this option: diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) + theme_classic() Figure: 5.8: A ggplot object with a geom_point layer, the price of diamonds by their carat, classic theme 5.2.3 Changing labels and titles Now we’ve changed axes, plot area, and gridlines, why don’t we give this beautiful plot some labels? If you were to take a wild guess at how to change labels - what would you add to the plot? Remember, taking the time to stop and try these exercises will help you learn R. And remember that R Studio will autocomplete things you type - what happens if you start to type ‘labels’? We can adjust labels with the following extra line of code. diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) + theme_classic() + labs (x = &quot;Carat (a measure of diamond weight)&quot;) Figure: 5.9: A ggplot object with a geom_point layer, the price of diamonds by their carat, adjusted labels Unsurprisingly, if we want to change the y axis label, we just need to add another argument inside the labs(). diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) + theme_classic() + labs (x = &quot;Carat (a measure of diamond weight)&quot;, y = &quot;Price in US dollars ($)&quot;) Figure: 5.10: A ggplot object with a geom_point layer, the price of diamonds by their carat, adjusted labels You can even add in titles and subtitles. diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) + theme_classic() + labs (x = &quot;Carat (a measure of diamond weight)&quot;, y = &quot;Price in US dollars ($)&quot;, title = &quot;Price of &gt;50,000 round-cut diamonds&quot;, subtitle = &quot;Data from ggplot2 package&quot;) Figure: 5.11: A ggplot object with a geom_point layer, the price of diamonds by their carat, adjusted labels 5.2.4 Adding regression lines to ggplot It seems as though the bigger the diamond is, the more you pay for it, so why don’t we add a line of best fit to demonstrate this? This is so easy to do in R. We add: We add to the graph a smooth line geom (stat_smooth) We have a number of options here: * We want the line to be calculated using a linear model (method = “lm”) We don’t want to see any standard error bars around the line (se = FALSE) diamonds |&gt; ggplot(aes(x = carat, y = price)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) + theme_classic() + labs (x = &quot;Carat (a measure of diamond weight)&quot;, y = &quot;Price in US dollars ($)&quot;, title = &quot;Price of &gt;50,000 round-cut diamonds&quot;, subtitle = &quot;Data from ggplot2 package&quot;) + stat_smooth(method = &quot;lm&quot;, se = FALSE) Figure: 5.12: A ggplot object with a geom_point layer, the price of diamonds by their carat, regression line added 5.2.5 Adding groups to ggplot Looking at our plot, it seems clear that the diamonds seem to stagger a bit, with lots of diamonds at 1, 1.5, 2, 3, and 3.5 carats, and fewer diamonds in the middle of a carat range. There may be something else in the data that helps to explain this . . . In ggplot, we can easily add a grouping variable to a scatterplot. All we need to do, is give it a new aesthetic (aes) argument: colour = cut. diamonds |&gt; ggplot(aes(x = carat, y = price, colour = cut)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) + theme_classic() + labs (x = &quot;Carat (a measure of diamond weight)&quot;, y = &quot;Price in US dollars ($)&quot;, title = &quot;Price of &gt;50,000 round-cut diamonds&quot;, subtitle = &quot;Data from ggplot2 package&quot;) + stat_smooth(method = &quot;lm&quot;, se = FALSE) Figure: 5.13: A ggplot object with a geom_point layer, the price of diamonds by their carat and cut, regression line added This has done quite a lot to our chart - its given us several new lines for each group, and a legend. If your computer is anything like mine, it might be starting to take a few seconds to render this chart. Let’s just do one more thing before we stop playing with this chart. 5.2.6 Changing legends Using the theme() argument (which is subtly different from the theme_classic() command), we can adjust the legend. diamonds |&gt; ggplot(aes(x = carat, y = price, colour = cut)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) + theme_classic() + labs (x = &quot;Carat (a measure of diamond weight)&quot;, y = &quot;Price in US dollars ($)&quot;, title = &quot;Price of &gt;50,000 round-cut diamonds&quot;, subtitle = &quot;Data from ggplot2 package&quot;) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(legend.position = &quot;bottom&quot;) Figure: 5.14: A ggplot object with a geom_point layer, the price of diamonds by their carat and cut, regression line added But lets say we also want to change the text from ‘cut’ to ‘Cut of Diamond’. Well, you can think of this as another axis we can change. Instead of a scale_x_ we can change the scale_colour_. And this isn’t a continuous scale but a discrete (categorical) one, so we change it with scale_colour_discrete. diamonds |&gt; ggplot(aes(x = carat, y = price, colour = cut)) + geom_point() + scale_x_continuous(breaks = seq (0, 6, 0.5), limits = c(0, 6)) + scale_y_continuous(breaks = seq(0, 20000, 2500), limits = c(0, 20000)) + theme_classic() + labs (x = &quot;Carat (a measure of diamond weight)&quot;, y = &quot;Price in US dollars ($)&quot;, title = &quot;Price of &gt;50,000 round-cut diamonds&quot;, subtitle = &quot;Data from ggplot2 package&quot;) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme(legend.position = &quot;bottom&quot;) + scale_color_discrete(name = &quot;Cut of Diamond&quot;) Figure: 5.15: A ggplot object with a geom_point layer, the price of diamonds by their carat and cut, regression line added Note that using scale_color_discrete has changed the way ggplot2 handles the default colour assignments for the factor. This might give you a clue as to where you might want to look to change the colours on purpose . . . 5.3 Building a boxplot At this stage, I’m wondering how useful our scatterplot is. Perhaps it would be easier to visualise this with a boxplot. We just need to build a new object. Lets look to see if there’s a relationship between how big the diamond is (carat) and its clarity (how clear it is). diamonds |&gt; ggplot(aes(x = clarity, y = carat)) + geom_boxplot() Figure: 5.16: A ggplot object with a geom_boxplot, the carat of diamonds by their clarity And with just a few lines of code, we can create a very different looking chart: diamonds |&gt; ggplot(aes(x = clarity, y = carat, colour = clarity)) + geom_boxplot() + labs(title = &quot;Clarity and carat of &gt;50,000 diamonds&quot;, subtitle = &quot;Data from ggplot2&quot;, x = &quot;Clarity (clearness) of diamond&quot;, y = &quot;Carat (weight) of diamond&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) Figure: 5.17: A ggplot object with a geom_boxplot, the carat of diamonds by their clarity 5.4 Facets Another very useful command is ‘facet’, which splits one chart into many based on a particular variable. diamonds |&gt; ggplot(aes(x = clarity, y = carat, colour = clarity)) + geom_boxplot() + labs(title = &quot;Clarity, cut and carat of &gt;50,000 diamonds&quot;, subtitle = &quot;Data from ggplot2&quot;, x = &quot;Clarity (clearness) of diamond&quot;, y = &quot;Carat (weight) of diamond&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) + facet_wrap(facets = ~cut) Figure: 5.18: A ggplot object with a geom_boxplot, the carat of diamonds by their clarity 5.5 Bar charts To create a bar chart, we simply need to change the geom_boxplot() to a geom_bar argument with a stat=\"summary\" specification. diamonds |&gt; ggplot(aes(x = clarity, y = carat, fill = clarity)) + geom_bar(stat = &quot;summary&quot;) + labs(title = &quot;Clarity, cut and carat of &gt;50,000 diamonds&quot;, subtitle = &quot;Data from ggplot2&quot;, x = &quot;Clarity (clearness) of diamond&quot;, y = &quot;Carat (weight) of diamond&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) + facet_wrap(facets = ~cut) Figure: 5.19: A ggplot object with a geom_bar, the carat of diamonds by their clarity There are a few notes here. Within geom_bar we have set the argument geom_bar(stat = \"summary\"). This tells R to calculate the mean carat for each group. Note as well that the groups are nested (we are calculating the mean carat for each clarity grouping inside each cut grouping). geom_bar wants fill = clarity instead of colour = clarity, as it treats colour as the line around the bar. If you’re anything like me you will always forget this and change the colour of the bar plot lines instead of its fill. 5.6 Exercise Please feel free to google and explore these questions - as well as putting your own customisation touches. Create a boxplot of the mpg cars dataset, plotting highway miles (hwy) against the car type (class) Answer here Create a histogram of the number of miles per gallon in the city (cty) faceted by type of transmission (trans) Answer here 5.7 Videos If you’d rather watch a video about this - you can here! 5.8 Useful resources There are some very useful resources out there about ggplot2 including: Cookbook for R, Winston Chang’s (free) online book about using ggplot ggplot cheatsheet, a pdf with lots of neat visualisations and cheats. "],["dataprocessing.html", "Chapter 6 Data Processing 6.1 Workflows 6.2 Opening R Markdown Docs 6.3 Running Code 6.4 Editing R Markdown Docs", " Chapter 6 Data Processing You can skip this chapter if you: Can download, open, edit and save an R Markdown file Can describe the advantages of repeatable workflows Can execute code in an R Markdown file 6.1 Workflows Throughout this book, I have been talking about the importance of repeatable workflows. If you’re on a course that I’m teaching, I will share R Markdown documents with you that you can edit and I can view. We could both start with the same dataset, and by sharing our workflow, we can both know exactly what the other person has done. This data transparency is very important. Knowing what changes have been made to the data, and how those changes were made, can help prevent small mistakes being magnified. For example, a simple copy/paste error in excel, resulted in an Edinburgh hospital being delivered over two years late. You might think this can’t apply to you, but think how many times you might make a change to a dataset in the process of cleaning that data and tidying it. If you had to describe what you did to a collaborator, would you remember every change? One of the great advantages of using code to process data is that every stage can be documented. In this textbook I’m teaching you R, and on my courses I mainly use R Markdown files to document workflows, but R is just one statistical language and R Markdowns are just one file type. The process is the important part. 6.1.1 An example of sharing code Imagine you have a big dataset like the UK Farm Accounts dataset: ## # A tibble: 6 × 6 ## Year `Gross output at basic prices £ million` Intermediate consumption £ millio…¹ Total Income from Fa…² Agriculture&#39;s share …³ Country ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1995 14741. 7491. 3669. NA England ## 2 1996 14954. 7900. 3436. NA England ## 3 1997 13334. 7472. 2028. NA England ## 4 1998 12329. 7008. 1473. NA England ## 5 1999 11926. 6876. 1535. NA England ## 6 2000 10963. 6483. 941. 1.32 England ## # ℹ abbreviated names: ¹​`Intermediate consumption £ million`, ²​`Total Income from Farming £ million`, ## # ³​`Agriculture&#39;s share of total regional employment %` And let’s say you want to explore the gross output of farming per year. You might decided it would be easier to change the name of the gross output variable to make coding easier. So you write the following in the console: farm &lt;- farm |&gt; rename(gross_output = &quot;Gross output at basic prices £ million&quot;) And then you summarise gross output by year: farm |&gt; group_by(Year) |&gt; summarise(mean_gross_output = mean(gross_output)) ## # A tibble: 25 × 2 ## Year mean_gross_output ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1995 4944. ## 2 1996 5010. ## 3 1997 4474. ## 4 1998 4115. ## 5 1999 3988. ## 6 2000 3744. ## 7 2001 3818. ## 8 2002 3876. ## 9 2003 4033. ## 10 2004 4198. ## # ℹ 15 more rows If you sent the above code chunk to me along with the original dataset, I would not be able to run it. I wouldn’t know what variable you had renamed. So at the start of the R Markdown document you would would want to load in the data and run any transformations/changes. That way if you sent me the RMD and the data file, I would be able to replicate what you did exactly. As a general practice, I aim to follow this structure in my RMD or R script files: Load any libraries that are needed Load the data Perform any data transformations Perform any analyses. There may be times when there’s a good reason to do things in a different order, but you will rarely go wrong with this order (Figure 6.1) Figure: 6.1: A general format of R Markdown files Sometimes in this book I have suggested using an R Script file instead of an R Markdown file. R Script files are good for short pieces of code, but if you want to talk about your code (say because you’re trying to ask your lecturer a question) you might find the ability to type in full paragraphs useful, and that’s where R Studio comes in. Some scientists will even write full papers in R Markdown, especially if there’s a lot of analyses in them. You can find out more about R Markdown formatting here. 6.2 Opening R Markdown Docs R Markdown files work just like any other file in R Studio. You can open any R Markdown file by going to File &gt; Open File ... and navigating to the folder your file is stored in. Your R Markdown file will ‘look’ for any linked files in the same folder its saved in, so if your data is saved in the same folder your R Markdown file is saved in, you can be quite lazy about your directories, e.g. read.csv(\"data.csv\"). This can be very helpful in repeatable analyses. Figure: 6.2: Opening an R Markdown file in R Studio 6.3 Running Code You can run code in R Markdown files by pressing the green ‘triangle’ button to run that specific piece of code. Figure: 6.3: Running a code chunk in an R Markdown file in R Studio If you have your cursor clicked inside a code chunk you can also press ‘ctrl + enter’ on your keyboard to run that specific line. 6.4 Editing R Markdown Docs You can edit both code and text chunks in R Markdown documents. A handy feature is the line numbers on the left-hand-side. You can tell your collaborator you’re having trouble with Line 100, which can help them help you! "],["tidyverse.html", "Chapter 7 The Tidyverse 7.1 Opinionated Packages 7.2 Data for this chapter 7.3 Mutating data 7.4 Wide and tall data", " Chapter 7 The Tidyverse You can skip this chapter if: You are comfortable using the tidyverse pivot_longer and pivot_wider commands You can rename variables You can create and change variables in a dataset. 7.1 Opinionated Packages Throughout this book I’ve been teaching you the tidyverse way of doing things. There’s quite a lot of debate as to whether tidyverse is the easy or hard way to learn things. A lot of people think that tidyverse is more difficult because it sometimes generates more lines of code. However, I really like the way that tidyverse code is easily guessed. If you know you want to change something, you can take a guess at what verb you want to use. This is because the tidyverse is ‘opinionated’. That means there’s an underlying philosophy behind how each package tries to think about data. I like the underlying theory, and I also like that the packages are explicit about the fact that data science itself comes with its own philosophies. One of the most important philosophies, as everyone online says - tidy data has one observation per row. There are a few things that tidyverse makes really easy: Visualising data with ggplot2 Making new variables or changing variables with mutate ‘Pivoting’ data into tall and wide formats with pivot We will cover these commands in this chapter. 7.2 Data for this chapter For this chapter let’s work on an example of student satisfaction data. We’ll use a short, fictional example to avoid embarrassing anyone. Let’s say I questioned my students on two courses, Professional Skills, an undergraduate course, and Research Methods, a postgraduate course. I know how many students (n) were in each class, and I asked each class if they agreed with two statements, “Jill was a good teacher” (good_teacher) and “I learned in this class” (learned). I know what percentage of students disagreed with the statement (disagree), were neutral about the statement (neutral), or agreed with the statement (agree). And I also know which of the two years I asked the question in (year). Let’s load the data and the tidyverse package first: library(tidyverse) students &lt;- tibble (course = c(&quot;Prof Skills&quot;, &quot;Prof Skills&quot;, &quot;Prof Skills&quot;, &quot;Prof Skills&quot;, &quot;Research Methods&quot;, &quot;Research Methods&quot;, &quot;Research Methods&quot;, &quot;Research Methods&quot;), level = c(&quot;UG&quot;, &quot;UG&quot;, &quot;UG&quot;, &quot;UG&quot;, &quot;PG&quot;, &quot;PG&quot;, &quot;PG&quot;, &quot;PG&quot;), question = c(&quot;good_teacher&quot;, &quot;learned&quot;,&quot;good_teacher&quot;, &quot;learned&quot;, &quot;good_teacher&quot;, &quot;learned&quot;,&quot;good_teacher&quot;, &quot;learned&quot;), year = c(1, 1, 2, 2, 1, 1, 2, 2), disagree = c(0.8, 0.3, 0.8, 0.2, 0.7, 0.5, 0.6, 0.3), neutral = c(0.05, 0.4, 0.1, 0.3, 0.1, 0.4, 0.2, 0.3), agree = c(0.15, 0.3, 0.1, 0.5, 0.2, 0.1, 0.2, 0.4), n = c(121, 121, 140, 140, 50, 50, 57,57)) 7.3 Mutating data We have covered the mutate function in previous chapters, but I’m going to specifically cover a few different forms of it now. In this section I’m going to create a new dataset students_tidy which will leave our original dataset students untouched. This is to demonstrate how much data can be transformed, and you might want to think about the difference between the original dataset and the finished product when you’re thinking about workflows. 7.3.1 Mutate to change a variable type. Let’s start with an example you’ve seen before. At the moment, year is a numerical variable, which we can prove: is.numeric(students$year) ## [1] TRUE So the first thing we want to do is make year a categorical variable, since there’s only two years available to us. We can retain the order of the levels by specifying them with the parse_factor command. parse_factor is really useful, but it only works on character variables, so we need to first change year to a character, and then to a factor. students_tidy &lt;- students |&gt; mutate(year = as.character(year), year = parse_factor(year, levels = c(&quot;1&quot;, &quot;2&quot;))) You can try taking out the year = as.character(year) line to see what happens. What error messages do you get? And now we can ask: is.factor(students$year) ## [1] FALSE is.factor(students_tidy$year) ## [1] TRUE 7.3.2 Mutate to change the contents of data What if we don’t want to change data type, but instead change the text of the data? There’s a very cool function called case_when which works like an if statement in Excel. students_tidy &lt;- students_tidy |&gt; mutate(level = case_when(level == &quot;UG&quot; ~ &quot;Undergraduate&quot;, level == &quot;PG&quot; ~ &quot;Postgraduate&quot;)) In this code chunk we: Create the object students_tidy (which we are overwriting, since it already exists) Make the new students_tidy object from the old one, and then … Change a variable within students_tidy (mutate) Create a new variable level (which we are overwriting, since it already exists) When a row of level reads UG, change it to (~) Undergraduate When a row of level reads PG, change it to (~) Postgraduate And we can check to see if this work by looking at a slice of the data. head(students_tidy) ## # A tibble: 6 × 8 ## course level question year disagree neutral agree n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Prof Skills Undergraduate good_teacher 1 80 5 15 121 ## 2 Prof Skills Undergraduate learned 1 30 40 30 121 ## 3 Prof Skills Undergraduate good_teacher 2 80 10 10 140 ## 4 Prof Skills Undergraduate learned 2 20 30 50 140 ## 5 Research Methods Postgraduate good_teacher 1 70 10 20 50 ## 6 Research Methods Postgraduate learned 1 50 40 10 50 7.3.3 Mutate to change multiple variables We can also change multiple variables using the mutate_at function. This can be a little more difficult to master, but is often faster than typing out multiple lines of mutate. Our disagree, neutral and agree columns are currently expressed as percentages, e.g. row 1 above had 80% of students disagreeing, 5% of students neutral, and 15% of students agreeing with the statement I was a good teacher. However, we know the number of students in each class, so it might be better to express those values as a proportion (e.g. 0.8, 0.05, 0.15). That’s a simple calculation - we need to take each value and divide by 100. To do this, we need to use two particularly cool things about tidyverse, the ability to select multiple variables, and the ability to use . to mean whatever I just asked for. students_tidy &lt;- students_tidy |&gt; mutate_at(.vars = vars(c(disagree, neutral, agree)), .funs = ~(. / 100)) In this code chunk we: Create the object students_tidy (which we are overwriting, since it already exists) Make the new students_tidy object from the old one, and then … Change more than one variable within students_tidy (mutate_at) Specify what variables we want to change (.vars = vars) List those variables, which are a string of names (c(disagree, neutral, agree)) Specify the function we want to apply to each of the previously selected variables (.funs =) We’re not asking for a named function so we show this with ~ We want to divide the previously asked for variables by 100 ((./100), where . is a dummy variable standing in for the previously selected variables. ) And as always, we can test this by showing a slice of the data: head(students_tidy) ## # A tibble: 6 × 8 ## course level question year disagree neutral agree n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Prof Skills Undergraduate good_teacher 1 0.8 0.05 0.15 121 ## 2 Prof Skills Undergraduate learned 1 0.3 0.4 0.3 121 ## 3 Prof Skills Undergraduate good_teacher 2 0.8 0.1 0.1 140 ## 4 Prof Skills Undergraduate learned 2 0.2 0.3 0.5 140 ## 5 Research Methods Postgraduate good_teacher 1 0.7 0.1 0.2 50 ## 6 Research Methods Postgraduate learned 1 0.5 0.4 0.1 50 7.3.4 Summarise as a unique form of mutate Mutate is really powerful thing, so unsurprisingly the idea behind it is used in other calls. One that’s really useful to know about (and that we’ll talk more about in descriptive statistics) is summarise. summarise creates a new mutated data frame by default, so its good for grouping together things, for example we can use it to look at the average percentage in each group by question: students_summed &lt;- students_tidy |&gt; group_by(question) |&gt; summarise(mean_disagree = mean(disagree), mean_neutral= mean(neutral), mean_agree = mean(agree)) And in fact, if we just want to look at this data quickly, we don’t even need to create a new dataset, we can just look at the output in the console: students_tidy |&gt; group_by(question) |&gt; summarise(mean_disagree = mean(disagree), mean_neutral= mean(neutral), mean_agree = mean(agree)) ## # A tibble: 2 × 4 ## question mean_disagree mean_neutral mean_agree ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 good_teacher 0.725 0.112 0.162 ## 2 learned 0.325 0.35 0.325 You’ll note that we lose all the other variables (like course, level and n) doing this, so you might want to be careful if you’re using summarise to make a new dataset. 7.4 Wide and tall data If you are looking at older materials they may use the terms gather instead of pivot_longer and spread instead of pivot_wider. In fact you can find a very similar version of the below text on my github page. This is a notable point about R - it is a language that is being actively used and changes as people use it. The idea is that pivot_longer is a more informative verb than gather, and so we should try to use that instead. At the moment both commands still work, but this may change in the years to come. 7.4.1 Processing data Let’s look at students_tidy again. head(students_tidy) ## # A tibble: 6 × 8 ## course level question year disagree neutral agree n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Prof Skills Undergraduate good_teacher 1 0.8 0.05 0.15 121 ## 2 Prof Skills Undergraduate learned 1 0.3 0.4 0.3 121 ## 3 Prof Skills Undergraduate good_teacher 2 0.8 0.1 0.1 140 ## 4 Prof Skills Undergraduate learned 2 0.2 0.3 0.5 140 ## 5 Research Methods Postgraduate good_teacher 1 0.7 0.1 0.2 50 ## 6 Research Methods Postgraduate learned 1 0.5 0.4 0.1 50 At first glance, this looks tidy. The data is presented with each course on a row - surely I’m observing at the course level? Well, actually, I probably often want to know what % of students agreed (or not) with each statement in each course. The observation in this case is actually the proportion of students, with question response, question, course, level, and year, all being extra pieces of information I know about the proportion. I want much taller data. (I’m using this specific example not because it’s a particularly easy example, but because this is a format you’ll see for data in the real world all the time, and people will make big decisions on this data. It’s a good idea to show you how to tidy it.) 7.4.2 pivot_longer The pivot_longer command is a quick way to smush this data into a tall (or long) format. It creates two new columns, the names_to column which collects your old column names and your values_to column which collects the row values (fairly self-explanatory). students_tall &lt;- students_tidy |&gt; pivot_longer(cols = c(disagree, neutral, agree), names_to = &quot;response&quot;, values_to = &quot;prop&quot;) This says: In the above code block we: Create a new dataset called students_tall students_tall is based on students_tidy We want to squish the data into new columns (pivot_longer) We specify the columns we want to stretch into two (cols = c(disagree, neutral, agree)) We specify the name for new column which will take the value of the old column headers (names_to = “response” - note we have to put quotation marks around the new name, which is not very common in tidyverse) We specify the name for the new column which will store the values of the old rows (values_to = “prop”) And of course, we can see what this has done to the data: head(students_tall) ## # A tibble: 6 × 7 ## course level question year n response prop ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Prof Skills Undergraduate good_teacher 1 121 disagree 0.8 ## 2 Prof Skills Undergraduate good_teacher 1 121 neutral 0.05 ## 3 Prof Skills Undergraduate good_teacher 1 121 agree 0.15 ## 4 Prof Skills Undergraduate learned 1 121 disagree 0.3 ## 5 Prof Skills Undergraduate learned 1 121 neutral 0.4 ## 6 Prof Skills Undergraduate learned 1 121 agree 0.3 It’s very important to think about your variable names I once spent a whole afternoon trying to recreate an error message I was getting with this, when I realised that I was saying names_to = “question”. The variable question already exists in the dataset, and so R was re-writing the variable every time it gathered the data. Unique variable names are really helpful! 7.4.3 pivot_wider What if, after all that, you realise that you never wanted your data gathered at all? pivot_wider is here to rescue you. Just as before, pivot_wider wants to know the names and the value, but this time, it will split those two columns into multiple columns. This time we want all that data to be spread out like marmalade on toast, so we don’t exclude any columns (in fact, try excluding the columns and see what spread says. ) students_wide &lt;- students_tall |&gt; pivot_wider(names_from = response, values_from = prop) And of course we can view this: head(students_wide) ## # A tibble: 6 × 8 ## course level question year n disagree neutral agree ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Prof Skills Undergraduate good_teacher 1 121 0.8 0.05 0.15 ## 2 Prof Skills Undergraduate learned 1 121 0.3 0.4 0.3 ## 3 Prof Skills Undergraduate good_teacher 2 140 0.8 0.1 0.1 ## 4 Prof Skills Undergraduate learned 2 140 0.2 0.3 0.5 ## 5 Research Methods Postgraduate good_teacher 1 50 0.7 0.1 0.2 ## 6 Research Methods Postgraduate learned 1 50 0.5 0.4 0.1 "],["trouble.html", "Chapter 8 Troubleshooting 8.1 Working independently 8.2 Common warnings 8.3 Common errors 8.4 Quick warnings and errors 8.5 Searching for help 8.6 Reproducible Examples (reprex)", " Chapter 8 Troubleshooting You can skip this chapter if: You understand the difference between an R warning and an R error You know the most common R errors You know where you would go for help 8.1 Working independently When you being working independently in R you will inevitably run into trouble. Maybe R doesn’t run the command you think it should, maybe it just gives you a warning. If something goes wrong, it does not mean you are bad at R. You have probably just made a typo, or forgotten a line of code. This isn’t a disaster. Everyone makes mistakes in R. The important thing is figuring out how to fix them. 8.2 Common warnings Sometimes (perhaps even ‘often’), R will give you a warning when you run a line of code. For example: warn &lt;- tibble(x = c(1,2,3,NA,1,2,3), y = c(2,3,1,2,3,1,1)) warn |&gt; ggplot(aes(x = x, y = y)) + geom_point() ## Warning: Removed 1 rows containing missing values (`geom_point()`). We receive the Warning: Removed 1 rows containing missing values (geom_point). In this case, its obvious that ggplot has had to get rid of the row of data with the NA value, and so we probably feel comfortable moving on with this warning. However, warnings can be a sign of something not behaving itself, and is usually a sign we should go back and double check our code. We might decide to move forward and ignore the warnings. 8.3 Common errors There are a lot of common errors in R, that can usually be easily fixed10. Errors mean that R has not been able to successfully run the code. 8.3.1 Object not found. Sometimes you might ask R to find something that doesn’t exist, for example: thing Error: object 'thing' not found The quote Error: object 'thing' not found tells us that R can’t find anything called ‘thing’ in the environment, or any of the loaded packages. Check that you’ve loaded all your required packages Make sure you’re spelling ‘thing’ right (and remember, R is case sensitive, ‘Thing’ might be right) Check you have definitely loaded all your datasets (if you’re working in an R Markdown document you may have forgotten to run the line that tells R what ‘thing’ is.) 8.3.2 Function not found Similarly, sometimes you might an error telling you that a function doesn’t exist. thing(1) Error in thing(1) : could not find function \"thing\" Again, you want to: Check you’ve definitely loaded the right packages Make sure you’re spelling ‘thing’ right (remembering R is case sensitive…) Make sure you definitely want to use a function. You can also get this error if you type x (1 / 2), even if you’ve predefined ‘x’ as a value. R thinks that opening brackets usually means using a function, so if you want to perform the calculation x * (1/2) you need to include the * symbol. 8.3.3 Unexpected symbols There are a few errors which are all similar, such as: mean(2,2,4,5,6)) Error: unexpected ')' in \"mean(2,2,4,5,6))\" and mean(2,2,3,5 6) Error: unexpected numeric constant in \"mean(2,2,3,5 6\" and mean(2,2,3;5,6) Error: unexpected ';' in \"mean(2,2,3;\" These are all clues to go back through the last run chunk of code and look for typos. 8.4 Quick warnings and errors There is a great resource from rex-analytics on errors and warnings, which I’ll display here: Figure: 8.1: Rex-analytics great infographic on R warnings and errors 8.5 Searching for help Of course sometimes you can’t figure out with what you already know. When this happens your next step is to google11. Many, many people know that they should just google things. These people have it cracked. If you know how to find an answer on Google, you will be immediately more employable than the person who doesn’t. This has even become a meme . . . Figure: 8.2: 10 years of programming, stolen from /r/programmerhumor Quite a few memes actually . . . Figure: 8.3: Jerry gets rewarded for googling, stolen from /r/programmerhumor One more to make the point . . . Figure: 8.4: Young Thug knows how to google, stolen from /r/programmerhumor The first step is to copy your error message and paste it into google. Scroll through the answers until you see one that looks similar to yours. Depending on how complex your code is, this might not be as simple as it seems. You may need to look at your error message for the key components. More often than not, you will probably find yourself on stackoverflow. I have personally asked questions on there. Lots of people make it their hobby to sit on stackoverflow and help people with their code. Sometimes you might find that the writer of the package you’re working with takes the time to answer. When you start to ask a question on stackoverflow, it gives you a really helpful set of tips. I have often started to write an answer and by the time I’ve worked through stackoverflow’s tips, I’ve figured out the answer. 8.5.1 Tips for asking for help Summarise the problem + What are you trying to do (what&#39;s your ideal output?) + What is happening at the moment (what&#39;s going wrong?) + What errors/warnings is R giving you? Describe what you’ve tried + What different methods have you tried? + (Hint: if you haven&#39;t tried anything it&#39;s going to annoy people) + Why do you think your attempts haven&#39;t worked? Show some code + Share a minimum reproducible example (reprex) so people can tell exactly what you&#39;re trying. Even if you don’t want to ask your question on stackoverflow, these tips can be really useful. 8.6 Reproducible Examples (reprex) I’ve talked a lot about reproducible examples or reprexes. Again, stackoverflow has great guidance on creating one of these. But we’ve actually already made one of these all the way back in chapter three when we were trying to figure out why summary was treating characters different from numerical variables. There are three key elements of a reprex: Be minimal * Use as little code as possible to reproduce the problem * You can do this in two main ways: Create a new file and add only what you need to reproduce the problem Remove code bit by bit from your file until you find the part that isn’t working In reprexes its often very useful to test on an inbuilt dataset in R, such as mpg or cars, because we all have access to these datasets, and it means you don’t have to send big data files to anyone (particularly if you’re working on data that shouldn’t be shared, e.g. personal data) Make sure your question is complete when you send it to someone * It sounds daft, but this happens more than you think. * The person trying to solve your problem should be able to replicate it using your code * Its a good idea to restart your R session, clear your environment, and run all your code fresh to check the problem still exists before sending it along. * Oh - and always send the actual code. Screenshots are not useful Describe the problem again before sending it * You may be surprised at how the problem has changed while troubleshooting! This section is nearly wholly stolen from David Robinson↩︎ or web search of your choice↩︎ "],["statsbasic.html", "Chapter 9 Basic Statistics 9.1 Descriptive stats 9.2 Median 9.3 Mode 9.4 Which model? 9.5 Measures of variation 9.6 Calculating descriptive stats 9.7 Reporting on heifer weight", " Chapter 9 Basic Statistics You can skip this chapter if … You can identify some common descriptive statistics You can report descriptive statistics You can describe the statistical formula for the mean and standard deviation 9.1 Descriptive stats Descriptive statistics, unsurprisingly, describe the data. But why do we need to describe data? Imagine that we had a field of cows. Figure: 9.1: A field of heifers We know that the first heifer has a weight of 211.3kg Figure: 9.2: A field of heifers, 1 weight known And that the second heifer has a weight of 200.4kg Figure: 9.3: A field of heifers, 2 weights known We can keep going, recording weights for every cow until we know all of them. Figure: 9.4: A field of heifers, all weights known They say that knowledge is power, and so now we know the weights of all these cows, we should be able to make better choices. Unfortunately, we often can’t make choices on an individual basis, so we need to make a choice that suits the most cows. How can we condense this information down? Statistics is really just condensing the real world down into simpler mathematical descriptions. 9.1.1 Mathematical notation I’ve changed my mind several times about whether I should teach mathematical notation when talking about statistics. I’ve decided to include it in this chapter, but if you find yourself glazing over, don’t worry too much. Sometimes I don’t bother teaching this. I’ve made the decision to talk about it because fundamentally, understanding what is happening when you calculate a mean (or a standard deviation) can help you understand some of the more complicated parts of statistics. You will likely never be quizzed on this, so I wouldn’t try to memorise them, but they can be a useful thing to know. 9.1.1.1 General notation Generally, in mathematical notation: Upper case letters denote a variable name (\\(X\\), \\(Y\\), \\(Z\\) …) Lower case letters denote a specific value of the variable (\\(x\\), \\(y\\), \\(z\\) …) Sample data is indicated with lower case letters (\\(v\\){\\(x_1\\), \\(x_2\\) … \\(x_n\\)}) So in this example we might say Let heifer weight be \\(X\\). 9.1.2 The mean Let’s create a dataset of heifer weight: coos &lt;- tibble(heifer_id = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), weight = c(211.3, 200.4, 220.1, 200.8, 222.0, 209.3, 195.8, 220.4, 225.2, 218.7, 193.7, 209.7)) We always want to visualise our data first - but just knowing weight isn’t that interesting: coos |&gt; ggplot(aes(x = heifer_id, y = weight, colour = as.factor(heifer_id))) + geom_point() + scale_y_continuous(limits = c(0, 250)) + scale_x_continuous(limits = c(0,12), breaks = seq(0,12,1)) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Weight (kg)&quot;, x = &quot;Heifer ID&quot;) Figure: 9.5: Weight (kg) of 12 heifers There’s not much that we can say about this chart. Most heifers seem to weigh around 207kg. But that’s an estimate that comes from me just eyeballing the data. We could draw a line at around 207kg, a line of best fit, which is a rough guess that tries to minimise the amount of distance between the line and each point. coos |&gt; ggplot(aes(x = heifer_id, y = weight, colour = as.factor(heifer_id))) + geom_point() + scale_y_continuous(limits = c(0, 250)) + scale_x_continuous(limits = c(0,12), breaks = seq(0,12,1)) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Weight (kg)&quot;, x = &quot;Heifer ID&quot;) + geom_segment(x = 0, xend = 12, y = 207, yend = 207) Figure: 9.6: Weight (kg) of 12 heifers, with a human-estimated best fit line We could use maths to draw that line in a better place . . . If we add up all the values of weight and divide by the number of cows . . . sum(coos$weight) / 12 ## [1] 210.6167 We get the exact same value as if we asked R to calculate the mean directly (using the handy summarise function): coos |&gt; summarise(mean(weight)) ## # A tibble: 1 × 1 ## `mean(weight)` ## &lt;dbl&gt; ## 1 211. 9.1.2.1 Notation of the mean We all know how to calculate the mean, but what does that look like in mathematical notation? The equation for the mean is: \\[\\overline{x} = \\frac{\\sum x^i}{n} \\] Let’s break this equation down: \\(\\overline{x}\\) is the mean of the sample \\(\\sum\\), the greek letter sigma means ‘add all this together’ \\(x^i\\) means ‘every data point in sample’ \\(n\\) means ‘number of observations in sample’. Figure: 9.7: Formula for the mean 9.1.3 The mean as a model So now we know that \\(\\overline{x}\\) = 210.6kg. We have created an imaginary model cow. This model cow has a weight of 210.6kg, and we know that she is not exactly real, but we can choose to make use of her, because she has a little bit of information from all of the samples. Figure: 9.8: The mean cow 9.2 Median There are other imagniary cows we could build to represent our dataset. coos |&gt; ggplot(aes(y = weight)) + geom_boxplot() + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Weight (kg)&quot;) Figure: 9.9: Weight (kg) distribution of 12 heifers The thick middle line in this boxplot represents the median cow. Figure: 9.10: The median cow Now the formula for the median is a little more complicated than I want to get into here, but the median essentially puts all of the cows in order: coos |&gt; arrange(weight) ## # A tibble: 12 × 2 ## heifer_id weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 11 194. ## 2 7 196. ## 3 2 200. ## 4 4 201. ## 5 6 209. ## 6 12 210. ## 7 1 211. ## 8 10 219. ## 9 3 220. ## 10 8 220. ## 11 5 222 ## 12 9 225. And then we pick the middle value. We have 12 cows, so our two middle values are 209.7 and 211.3. So to find the median, we add those two values together and divide by 2. coos |&gt; summarise(median_weight = median(weight)) ## # A tibble: 1 × 1 ## median_weight ## &lt;dbl&gt; ## 1 210. 9.3 Mode The final simple model we will talk about is the mode, or the most frequent value in a dataset. The mode isn’t that useful, and so there isn’t actually an inbuilt function to visualise or calculate it. mode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } coos |&gt; summarise(mode_weight= (mode(weight))) ## # A tibble: 1 × 1 ## mode_weight ## &lt;dbl&gt; ## 1 211. The mode is a bit . . . useless . . . but it is still a model cow, so it also gets its own picture in this example. Figure: 9.11: The modal cow 9.4 Which model? The mean, mode and median cow all try to give a good approximation of most cows in our field. We know that they can’t describe every cow in detail (because we don’t want to have to learn every single cow’s weight off by heart), but they all have to lose important information. With each of these models - we hope that our calculated weight is close to most of the population. We know that there is always natural variation in the world, and some cows might be very heavy, and some cows might be very thin, but our hope is that our models are fair representations of most cows in our dataset. But each model takes a different approach to this. The mean is also described as the central tendency, and it takes into account every single individual in the sample. However, if you have a very skinny or very heavy cow, they will (by virtue of how the mean is calcualted) have a big impact on the final value. The median takes the ‘middle’ value of the dataset, and can be very useful when you have extreme outliers in a dataset, because it essentially ignores those values. The median is sometimes referred to as the most resilient statistic because so long as at least 50% of your values are normal, the median will be close to at least 50% of your individuals. The mode, as the most common value in the dataset, but this is rarely very useful in biological data. What if we had two very small cows who weighed 100kg, and every other cow of normal weight had a unique value? The mode would make its imaginary cow 100kg, even though every other cow in the dataset was over 100kg heavier. However, you can use the mode for nominal data, which can be useful, e.g. what is the most common name of cow? 9.5 Measures of variation In the previous section we talked about outliers, and how they can affect these three models of our cows. We’ve already looked at ways to visualise distributions, but lets remind ourselves: coos |&gt; ggplot(aes(x = weight)) + geom_histogram(binwidth = 10) + theme_classic() Figure: 9.12: A histogram of heifer weight The histogram shows us how many cows are in each weight category (or ‘bin’). We can overlay the mean, median and mode on this, to see which one gets closest to the ‘middle’. coos |&gt; ggplot(aes(x = weight)) + geom_histogram(binwidth = 10) + theme_classic() + geom_segment(x = 210.6, xend = 210.6, y = 0, yend = 4, colour = &quot;pink2&quot;) + geom_segment(x = 210.5, xend = 210.5, y = 0, yend = 3.5, colour = &quot;yellow1&quot;) + geom_segment(x = 211.3, xend = 211.3, y = 0, yend = 4, colour = &quot;lightblue1&quot;) Figure: 9.13: A histogram of heifer weight(kg), mean, median and mode shown Now these three lines (the median (yellow), mean (pink) and mode (blue)) are all very close to one another. So close in fact that the mean and median are nearly on top of one another. This suggests to us that our field of heifers is fairly normal. Most observations are around the mean, and so the mean and the median are fairly similar. We have very few unusual observations (very few underweight or overweight cows). Again, this is just us eyeballing the data. It can be useful to put numbers on how variable a dataset is. 9.5.1 Variance Variance (\\(\\sigma^2\\)) describes how far away each observation in our sample is from the sample mean. If we drew a line representing the mean (\\(\\overline{x}\\)) on our scatteplot of each individual value, we could measure how far away each point was from the line. If we added all these numbers together they would sum to zero (if you want to see a proof of this jump here). coos |&gt; ggplot(aes(x = heifer_id, y = weight, colour = as.factor(heifer_id))) + geom_point() + scale_y_continuous(limits = c(0, 250)) + scale_x_continuous(limits = c(0,12), breaks = seq(0,12,1)) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Weight (kg)&quot;, x = &quot;Heifer ID&quot;) + geom_segment(x = 0, xend = 12, y = 210.6, yend = 210.6, colour = &quot;pink2&quot;) Figure: 9.14: Weight (kg) of 12 heifers, with a mean line So we need to transform the distances by squaring them. Now, if we were dealing with hundreds of observations we would of course have a much bigger number, so we need to standardise them by the number of observations (just like we do with the mean calculation). We can calculate the variance of the cows’ weight very easily: coos |&gt; summarise(variance = var(weight)) ## # A tibble: 1 × 1 ## variance ## &lt;dbl&gt; ## 1 118. 9.5.1.1 Formula for variance Variance is calculated like this: \\[\\sigma^2 = \\frac{\\sum(x-\\overline{x})^2}{n-1} \\] Breaking this equation down … \\(\\sigma^2\\) is the symbol for variance \\(\\sum(x-\\overline{x})^2\\) means ’take each value of \\(x\\) and subtract the mean (\\(\\overline{x\\)}). Square this value (\\(^2\\)) and add them all together (\\(\\sum\\)) Divide this value by the total number in the sample, minus 1 (\\(n-1\\)). We say \\(n-1\\) because we are estimating the variance of a sample population1 9.5.2 Standard Deviation The standard deviation (\\(\\sigma\\)) is very similar to variance (\\(\\sigma^2\\)). Can you guess what the difference is? The standard deviation is the square root of the variance. When we calculate the variance, we simply sum all the squares together, but we really want to express this number in the same unit as the original measurements (not the square of the measurements). So if we take the square root of the variance, we calculated the standard deviation. This makes the standard deviation a really useful add-on to the mean, because its expressed in the same units. We can of course calculate this: coos |&gt; summarise(standard_deviation = sd(weight)) ## # A tibble: 1 × 1 ## standard_deviation ## &lt;dbl&gt; ## 1 10.9 9.5.2.1 Formula for standard deviation. \\[\\sigma = \\sqrt \\frac{\\sum(x-\\overline{x})^2}{n-1} \\] Breaking this equation down … \\(\\sigma\\) is the symbol for standard deviation \\(\\sum(x-\\overline{x})^2\\) means ’take each value of \\(x\\) and subtract the mean (\\(\\overline{x}\\)). Square this value (\\(^2\\)) and add them all together (\\(\\sum\\)) Divide this value by the total number in the sample, minus 1 (\\(n-1\\)) Take the square root (\\(\\sqrt\\)) of this value. 9.5.3 Standard Error The third and final measure of variance I’ll discuss in this chapter is the standard error (\\(\\sigma_\\overline{x}\\)). The standard error is an attempt to describe how far away the sample mean (\\(\\overline{x}\\)) is from the true population mean (\\(\\mu\\))? In our cows example, we’ve measured 12 cows, but we know this is not all the cows in the world. If, in theory, we could measure every single cow in the world, we could calculate the variance of the population (we wouldn’t need to say \\((n-1)\\) in our variance calculation because we would know exactly how many cows we had). \\[\\sigma_{population} = \\sqrt \\frac{\\sum(x-\\overline{x})^2}{n_{population}} \\] In this case \\(n_{population}\\) is every single cow in the world. So if we know how many cows are in our sample, we could calculate the variance of our sample (\\(\\sigma^2_{sample}\\)) in a slightly different way: : \\[\\sigma^2_{sample} = \\frac{\\sigma^2_{population}}{n_{sample}}\\] Now we hardly ever calculate our sample variance this way because we are rarely ever in a position where we know every single value. Remember that above we saw that the standard deviation (\\(\\sigma\\)) was the square root of the variance (\\(\\sigma^2\\)). The standard error is the standard deviation divided by the square root of the sample size. This is a bit complicated - and harks back to old algebra lessons about moving variables from one side of an equation to another, and you can read more about it on wiki. All you need to know is this: The standard error of the mean is the standard deviation of its sampling distribution. The standard error of the mean is an estimate of how good the mean is at describing the whole population. It will always be smaller than the standard deviation. 9.5.3.1 Formula for the standard error of the mean The standard error of the mean is calculated like this: \\[\\sigma_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}} \\] Breaking this equation down … \\(\\sigma_{\\overline{x}}\\) is the symbol for the standard error of the mean \\(\\sigma\\) is the standard deviation of the sample Divide this value by the square root of the total number in the sample (\\(\\sqrt{n}\\)) And like always, we can calculate this, although there’s no inbuilt function to do this, we know the equation is pretty simple: coos |&gt; summarise (standard_error_mean = sd(weight)/sqrt(length(weight))) ## # A tibble: 1 × 1 ## standard_error_mean ## &lt;dbl&gt; ## 1 3.14 9.6 Calculating descriptive stats So with our cows, we can calculate lots of different values that each say something a little bit different about our dataset. On their own, these numbers are not always useful, but its important to recognise the relationship between a lot of these calculations. Lets calculate all of these things in one table: coos |&gt; summarise(median_weight = median(weight), mode_weight = mode(weight), mean_weight = mean(weight), variance_weight = var(weight), sd_weight = sd(weight), sem_weight = sd(weight)/sqrt(length(weight))) ## # A tibble: 1 × 6 ## median_weight mode_weight mean_weight variance_weight sd_weight sem_weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 210. 211. 211. 118. 10.9 3.14 9.7 Reporting on heifer weight If we wanted to report on heifer weight, we would want to give our best description of the data, which is usually the mean and standard deviation of a normally distributed dataset. And it’s always a good idea to point your reader towards a visualisation of the data. To report any statistic, you first describe the result in plain English, and then, in brackets, add the proof (which is usually the statistical result and/or a figure.) The type of model you choose is entirely up to you. There’s no ‘perfect’ answer, just the one that best represents your data. Heifers in this field weighed on average 210.6kg +/- 10.88kg (median = 210.5kg, Figure 9.15) coos |&gt; ggplot(aes(y = weight)) + geom_boxplot() + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(y = &quot;Weight (kg)&quot;) Figure: 9.15: Weight (kg) distribution of 12 heifers, mean = 210.6kg +/- 10.88kg Footnotes might contain something useful - or something silly. I guess you’ll need to click it to see! But remember, you can’t trust me implicitly - or anyone really↩︎ "],["statsfreq.html", "Chapter 10 Predicting more things 10.1 Partitioning Variation", " Chapter 10 Predicting more things You can skip this chapter if . . . You can describe the basic theory of partitioning variation You can run an ANOVA or glm in R 10.1 Partitioning Variation In the previous chapter, we spent a lot of time defining variance. Part of the reason why we spent so much time on it is because its key to understanding how some more complicated statistics work. Much of this section is taken from Grafe &amp; Hails’ Modern Statistics for the Life Sciences which I think is a great stats book. If you’re enjoying statistics, or want more detail than is in these pages, its one that I personally used a lot when I was learning. In this exercise, we’re going to spend more time thinking about variance - and we need to have tidyverse loaded, so you know what to do: library(tidyverse) 10.1.1 Perfect Worlds vs Reality Let’s think a little bit about what we mean by this question: does x affect y? We want to know what proportion of the variation we observe in y can be explained by x. To do this, we want to think about a perfect example where all of the variation in y can be explained by x. Let’s create some data: examples &lt;- tibble (x.perfect = c(1,2,3,4,5,6,7,8,9,10), y.perfect = c(1,2,3,4,5,6,7,8,9,10), x.realistic = c(1,2,3,4,4,6,8,8,9,11), y.realistic = c(2, 2, 4, 5, 5, 5, 7, 8, 9, 9)) First, let’s visualise the perfect world: examples |&gt; ggplot (aes(x = x.perfect, y = y.perfect))+ geom_point () + scale_y_continuous(limits =c(0,10)) + scale_x_continuous(limits = c(0,10)) + labs (title = &quot;100% of the variation in y is explained by x&quot;) + theme_classic() Figure: 10.1: 100% of the variation in y is explained by x In a ‘perfect’ dataset, we see that for every unit increase in x, y increases by the same. There’s a 1:1 relationship between the two. Of course, we very rarely ever see this in the real world. How does it look with (slightly) more realistic data? examples |&gt; ggplot (aes(x = x.realistic, y = y.realistic))+ geom_point () + scale_y_continuous(limits =c(0,10)) + scale_x_continuous(limits = c(0,10)) + labs (title = &quot;How much of the variation in y is explained by x?&quot;) + theme_classic() Figure: 10.2: A more realistic example Most of us looking at this chart would be able to draw a ‘line of best fit’ describing the relationship between x and y in this example. With a linear model we can calculate this exactly. This is the exact same equation as you would have learned at school: \\[ y = mx + c \\] or predicted y = gradient * (a value of x) + y intercept We can ask R to fit a linear model for this data. The function lm asks for a formula argument which looks like y ~ x. The formula is a bit unusual in terms of R language because there are no ‘placeholder’ or ‘default’ values, meaning we can’t type y = y.realistic here. The lm function is part of R’s basic stats code (meaning we don’t need to load any particular library to use it). ## ## Call: ## lm(formula = y.realistic ~ x.realistic, data = examples) ## ## Coefficients: ## (Intercept) x.realistic ## 1.3659 0.7561 The output here starts with the Call which reiterates what we specificed in our lm command. Then it gives us: Coefficient of the intercept (where our model line crosses the y axis) Coefficient of x.realistic Since x.realistic is a continuous variable, the x.realistic coeffcient represents the difference in the predicted value of Y for each one-unit difference in x.realistic. If that’s confusing, let’s run this on the perfect data: ## ## Call: ## lm(formula = y.perfect ~ x.perfect, data = examples) ## ## Coefficients: ## (Intercept) x.perfect ## 1.123e-15 1.000e+00 The coefficient for x.perfect is 1. For every 1 step increase in the value of x, our value of y increases by 1. Our coefficient is very nearly zero (it’s not exactly zero because of a thing called floating point math which is kind of interesting but not super relevant). We could plug this in to our equation above to predict a value of y. The coefficient of x is the same as the gradient in a simple linear model, and so we can predict y when x = 3: \\[ y = mx + c \\] predict.perfect.y = 1 * 3 + 0 predict.perfect.y ## [1] 3 Perfect! What if we try our realistic data? If you View(examples) you can see that when x.realistic = 3, y.realistic = 4. Does our linear model predict this? predict.realistic.y = 0.7561 * 3 + 1.3659 predict.realistic.y ## [1] 3.6342 … Sort of? It doesn’t predict it perfectly, but its pretty close . . . You might now be wondering, is this significant? You’ll have heard scientists talk about significance a lot. Well, we can find this out by asking R to summarise the model: ## ## Call: ## lm(formula = y.realistic ~ x.realistic, data = examples) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9024 -0.6159 0.1220 0.6037 0.8293 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.36585 0.46584 2.932 0.0189 * ## x.realistic 0.75610 0.07258 10.418 6.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7199 on 8 degrees of freedom ## Multiple R-squared: 0.9314, Adjusted R-squared: 0.9228 ## F-statistic: 108.5 on 1 and 8 DF, p-value: 6.247e-06 This time we get more information. The coefficients are still listed in the table, alongside their standard error, a T Value and a P value. The P value is very small, much smaller than the cut-off of p&lt;0.05. Therefore we must say this is a significant result? At the top we have residuals which we’ll come to another day, and at the bottom we have an F statistic and a P Value (these are the ones I’d pay attention to if I were you) and an Adjusted R-squared. At the moment this tell us that 92% of the variation in y.realistic can be explained by x.realistic. And if we wanted, we could add this to our plot from earlier with the addition of a single line of code: examples |&gt; ggplot (aes(x = x.realistic, y = y.realistic))+ geom_point () + scale_y_continuous(limits =c(0,10)) + scale_x_continuous(limits = c(0,10)) + labs (title = &quot;How much of the variation in y is explained by x?&quot;) + theme_classic() + # And add a single line below: geom_smooth (method = &#39;lm&#39;) Figure: 10.3: y.realstic and x.realistic with a fit line If we were reporting this we would say that: y.realistic increased with x.realistic. A linear model predicted that for every unit increase in x.realistic, y.realistic would increase by 0.76, and this explained 93% of the variation observed in y.realistic (F1,8 = 108.5, p &lt; 0.001, Figure 10.3) But what do all these numbers mean? That’s what we’re going to look at: 10.1.2 A Fake Experiment We’re going to start with a fake experiment to explore this. As you may know, the national animal of Scotland is the unicorn. In our example, Scotland has started farming unicorns (in a very welfare friendly manner, of course) to export more magic dust. Unicorns shed magic dust from their horns every morning, and this is gathered by the farmer. There’s a belief that unicorns shed more dust when they hear music, and so we set up a trial with three farms: Farm 1 was our negative control farm, with no intervention. Farm 2 was our trial farm, where a radio station tuned to music was played over the speakers. Farm 3 was a positive control farm, and played radio tuned to BBC Radio 4 for thoughtful discussion. We recorded the raw weekly dust yield of 10 unicorns on each farm, and the data looks like this: Radio &lt;- tibble (NoRadio = c(150, 130, 121, 90, 98, 100, 98, 100, 113, 111), RadioMusic = c(112, 127, 132, 150, 112, 128, 110, 120, 98, 107), RadioDiscussion = c(75, 98, 88, 83, 83, 77, 75, 84, 93, 99)) In the majority of our experiments and statistical analyses, we always want to know Does x affect y?. In this case, we’re asking: Does radio as a background noise affect the magic dust yield of unicorns? 10.1.2.1 Tidying this data Remember that we want data to be tidy. We should pivot this data so every row is an observation of a unicorn. Try creating a new dataset caled RadioTidy yourself. RadioTidy should have two columns named “Radio” and “DustYield”. You can check your work in the answers If you use your own solution to tidy the data remember that R is case sensitive. I have called the columns Radio and DustYield. If your data isn’t named in the same way copying and pasting the following code chunks will give you errors. 10.1.2.2 Visualising our unicorns We know we should start by visualising our data, but in this example I’m going to visualise it in a few different ways. You wouldn’t normally do this, but I’m trying to demonstrate some things about variance. RadioTidy |&gt; ggplot (aes (x = Radio, y = DustYield)) + geom_point(aes(shape = Radio, colour = Radio)) + labs (y = &quot;Dust Yield (Kg)&quot;, x = &quot;Radio Condition&quot;) + scale_y_continuous(limits = c(0, 200)) + theme_classic () Figure: 10.4: Magic Dust Yield (kg) by Unicorn Farm In this chart we have plotted the yield of every single unicorn by farm. We still want to know whether radio affects yield, but we know that some unicorns are probably high dust yielders, and others might be low yielding unicorns. In other words, we know that individuals vary, and so we want to know what happens to the group on average. But we can also see here that the groups seem to be different. The amount of variation isn’t the same between the groups. In fact, the farm ‘Radio Discussion’ is more closely clustered together than the others. The variance of a dataset describes how much the data is scattered around the mean. An ANOVA is simply asking: &gt;Is the mean of each group a better predictor than the mean of all the data?. Let’s start by looking at only one farm (note - you wouldn’t be likely to do this in a real life example, this is for explanation purposes only): Radio |&gt; mutate(UnicornNo = c(1,2,3,4,5,6,7,8,9,10)) |&gt; # The mutate function adds a new variable just to plot this one specific chart # And then we pipe it directly into ggplot, so we&#39;re not changing the Radio data # Remember you can check this with `View(Radio)`, you&#39;ll see &#39;UnicornNo&#39; doesn&#39;t exist. ggplot (aes (x = UnicornNo, y = NoRadio)) + geom_point() + labs (y = &quot;Dust Yield (Kg)&quot;) + scale_y_continuous(limits = c(0, 200)) + scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) + theme_classic () Figure: 10.5: Magic dust yield (kg) for Farm 1 Deviations from the mean will be both positive and negative, and the sum of these deviations will always be zero. First, let’s find out the mean dust yield of the farm without any radio . . . RadioTidy |&gt; group_by(Radio) |&gt; filter(Radio == &quot;NoRadio&quot;) |&gt; summarise(mean = mean(DustYield)) ## # A tibble: 1 × 2 ## Radio mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 NoRadio 111. We could plot this mean on our chart with an extra line of code: Radio |&gt; mutate(count = c(1,2,3,4,5,6,7,8,9,10)) |&gt; ggplot (aes (x = count, y = NoRadio)) + geom_point() + labs (y = &quot;Dust Yield (Kg)&quot;) + scale_y_continuous(limits = c(0, 200)) + theme_classic () + # This new line basically draws a line on our chart based on the mean we just calculated geom_hline(yintercept = 111.1) Figure: 10.6: Magic dust yield (kg) for Farm 1, Mean Line Added 10.1.2.3 Deviation from the group mean We want to know what the deviation from the mean of the group is for each individual. We can ask R to calculate this for us using the handy mutate function: Radio |&gt; mutate (NoRadioDeviation = (mean(NoRadio) - NoRadio)) ## # A tibble: 10 × 4 ## NoRadio RadioMusic RadioDiscussion NoRadioDeviation ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 150 112 75 -38.9 ## 2 130 127 98 -18.9 ## 3 121 132 88 -9.9 ## 4 90 150 83 21.1 ## 5 98 112 83 13.1 ## 6 100 128 77 11.1 ## 7 98 110 75 13.1 ## 8 100 120 84 11.1 ## 9 113 98 93 -1.90 ## 10 111 107 99 0.100 You can check that the sum of NoRadioDeviation does add to zero (or at least very close to it given some rounding errors because of floating point math): Radio |&gt; mutate (NoRadioDeviation = (mean(NoRadio) - NoRadio)) |&gt; summarise (&quot;Sum of Deviations from No Radio Mean&quot; = sum(NoRadioDeviation)) ## # A tibble: 1 × 1 ## `Sum of Deviations from No Radio Mean` ## &lt;dbl&gt; ## 1 -5.68e-14 No matter what your dataset looks like, no matter what numbers are in there, this will always be true. 10.1.2.4 Calculating variance from deviations Because the deviations will always sum to 0, the raw deviations are not very useful. So how can we compare the deviation (variance) between two datasets? If we square the deviations and then sum them we have a more useful measure of variance that we call Sums of Squares (SS) Radio |&gt; mutate (NoRadioDeviation = (mean(NoRadio) - NoRadio), SquaredNoRadioDeviation = (NoRadioDeviation*NoRadioDeviation)) ## # A tibble: 10 × 5 ## NoRadio RadioMusic RadioDiscussion NoRadioDeviation SquaredNoRadioDeviation ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 150 112 75 -38.9 1513. ## 2 130 127 98 -18.9 357. ## 3 121 132 88 -9.9 98.0 ## 4 90 150 83 21.1 445. ## 5 98 112 83 13.1 172. ## 6 100 128 77 11.1 123. ## 7 98 110 75 13.1 172. ## 8 100 120 84 11.1 123. ## 9 113 98 93 -1.90 3.61 ## 10 111 107 99 0.100 0.0100 Sums of Squares are useful because they don’t sum to 0, but they’re still influenced by the number of data points we have (e.g. if we had one more unicorn in the No Radio farm the sum of squares would have to increase). So we calculate the variance of the dataset by: \\[\\sigma^2 = \\frac{SumsofSquares}{n-1} \\] $$ Variance is therefore a measure of the variability of a dataset that takes the size of the dataset into account. And we can use variance to compare variability across different datasets. It’s very useful! R, we know, has a very handy function to calculate variance automatically: Radio |&gt; summarise(&quot;Variance No Radio&quot; = var(NoRadio)) ## # A tibble: 1 × 1 ## `Variance No Radio` ## &lt;dbl&gt; ## 1 334. But if you’re not convinced . . . Radio |&gt; mutate (NoRadioDeviation = (mean(NoRadio) - NoRadio), SquaredNoRadioDeviation = (NoRadioDeviation*NoRadioDeviation)) |&gt; summarise(SumSquaresNoRadio = sum(SquaredNoRadioDeviation)) ## # A tibble: 1 × 1 ## SumSquaresNoRadio ## &lt;dbl&gt; ## 1 3007. And then divide that by n-1: 3006.9/9 ## [1] 334.1 10.1.2.5 Partitioning Variation Remember, in this experiment we can see that there is variation around dust yields. What we want to know is: How much of this variation is due to our explanatory variable (what kind of radio they listened to) How does x affect y? 10.1.2.5.1 Imaginary Scenario 1 Let’s imagine a scenario where the condition (our categorical variable) explains almost all of the variance in the dataset. It would look like this: Sce1 &lt;- tibble (Condition1 = c(99,100,101,99,100,101,99,101,100,101), Condition2 = c(120,121,122,120,121,122,120,123,121,120), Condition3 = c(83,84,85,85,84,83,83,84,85,86)) |&gt; pivot_longer (cols = c(Condition1:Condition3), names_to = &quot;Condition&quot;, values_to = &quot;DustYield&quot;) |&gt; mutate (Count = c(1:30)) # In the code above I&#39;ve gathered the data into a tidy format ImaginaryScenario1 &lt;- Sce1 |&gt; ggplot (aes (x = Count, y = DustYield)) + geom_point(aes(shape = Condition, colour = Condition)) + labs (y = &quot;Dust Yield (Kg)&quot;, x = &quot;Unicorn ID Number&quot;) + scale_y_continuous(limits = c(0, 200)) + theme_classic () # I have also made this chart an object because we&#39;re going to update it # It&#39;s quicker to do this as an object # You can compare how we update this chart with how we update the ones above. ImaginaryScenario1 We could fit the mean of the dataset on this: ImaginaryScenario1 + geom_hline(yintercept = mean(Sce1$DustYield)) Let’s compare the overall mean with the group means, and see which one is the best guess for any given data point. Remember - if the condition is explaining a lot of the variation around our response, we’ll see less deviation from each group mean than the overall mean. Sce1 |&gt; group_by(Condition) |&gt; summarise(mean = mean (DustYield)) ## # A tibble: 3 × 2 ## Condition mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Condition1 100. ## 2 Condition2 121 ## 3 Condition3 84.2 ImaginaryScenario1 + geom_hline(yintercept = mean(Sce1$DustYield)) + geom_segment(aes(x =1, y = 100.1, xend =10, yend = 100.1, color = &quot;red&quot;)) + geom_segment(aes(x = 11, y = 121.0, xend = 20, yend = 121.0, color = &quot;green&quot;)) + geom_segment (aes(x = 21, y = 84.2, xend = 30, yend = 84.2, color = &quot;blue&quot;)) + theme (legend.position = &quot;none&quot;) In this case, the individual group means are a better description of the group than the overall population mean. Any statistics we were to apply would simply put a number to what we see on this chart. So let’s look at a second scenario: Sce2&lt;-tibble (Condition1 = c(84,86,123,95,87,110,99,95,121,121), Condition2 = c(83,115,85,85, 110,105,84,115,101,100), Condition3 = c(84,122,80,80,101,83,83,99, 120,120)) |&gt; pivot_longer (cols = c(Condition1:Condition3), names_to = &quot;Condition&quot;, values_to = &quot;DustYield&quot;) |&gt; mutate (Count = c(1:30)) Sce2 |&gt; group_by(Condition) |&gt; summarise(mean = mean (DustYield)) ## # A tibble: 3 × 2 ## Condition mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Condition1 102. ## 2 Condition2 98.3 ## 3 Condition3 97.2 Now we chart it, and add our segment lines for each group’s mean: Sce2 |&gt; ggplot (aes (x = Count, y = DustYield)) + geom_point(aes(shape = Condition, colour = Condition)) + labs (y = &quot;Dust Yield (Kg)&quot;, x = &quot;Unicorn ID Number&quot;) + scale_y_continuous(limits = c(0, 200)) + theme_classic () + geom_hline(yintercept = mean(Sce2$DustYield)) + geom_segment(aes(x =1, y = 102.1, xend =10, yend = 102.1, color = &quot;red&quot;)) + geom_segment(aes(x = 11, y = 98.3, xend = 20, yend = 98.3, color = &quot;green&quot;)) + geom_segment (aes(x = 21, y = 97.2, xend = 30, yend = 97.2, color = &quot;blue&quot;)) + theme (legend.position = &quot;none&quot;) In this second scenaro, the population mean is not very different from the means of each of the three conditions. In fact, it’s hard to see some of those means on the chart! We haven’t successfully partitioned off any of the dataset’s varaiance by looking at each of the condition means. In this second scenario, the explanatory variable is not explaining very much of the response variable at all. 10.1.2.6 Our Scenario If we plot our unicorn data in the same format: RadioTidy |&gt; mutate(count = c(1:30)) |&gt; ggplot (aes (x = count, y = DustYield)) + geom_point(aes(shape = Radio, colour = Radio)) + labs (title = &quot;Magic Dust Yield(kg)&quot;, y = &quot;Dust Yield (Kg)&quot;, x = &quot;Unicorn ID Number&quot;) + scale_y_continuous(limits = c(0, 200)) + theme_classic () 10.1.2.6.1 Going Back to Sums of Squares In our two imaginary scenarios above we were (roughly) partitioning variances by drawing lines on our charts - much like we could draw a line of best fit on the perfect data chart earlier. When we run a model, we want to describe that chart mathematically. When we calculated variance earlier, we calculated the variance in the condition mean mean (e.g. the variance for No Radio data). We can call this the Error Sum of Squares (SSE) which is the sum of the squares of the deviations of the data around the means of each group. We could also calculate … Total Sums of Squares (SSY), the sums of squares of the deviations of the data around the population mean. Condition Sums of Squares (SSF), the sums of squares of the deviations of each condition mean from the population mean. But why would we use any sums of squares when we already establishd that variance was a better measure? Let’s rearrange our data so we have for each ‘row’ the condition mean and total mean. We’re going to call this dataset MFY for short because: M is the global mean F is the condition mean Y is the ‘response variable’ MFY &lt;- RadioTidy |&gt; mutate (&quot;Y&quot; = DustYield) |&gt; mutate (&quot;M&quot; = mean(Y)) |&gt; # If we now ask R to group the data, it will calculate the mean per group: group_by(Radio) |&gt; mutate (&quot;F&quot; = mean(Y)) |&gt; # Remember to ungroup after! ungroup() # I suggest you View(MFY) We can also calculate: MY (The data - Dataset mean) MF (The condition mean - dataset mean) FY (The data - condition mean) MFY &lt;- MFY |&gt; mutate (MY = (Y-M), MF = (F-M), FY = (Y - F)) We’ve calculated a lot of deviations here - but we said we always wanted to square those deviations, so: MFY &lt;- MFY |&gt; mutate (MY2 = (MY*MY), MF2 = (MF*MF), FY2 = (FY*FY)) MY2, MF2 &amp; FY2 are the squares of the deviations, but we need a sum of squares for each one. MFY |&gt; summarise(SumSquareMY = sum(MY2), SumSquareMF = sum(MF2), SumSquareFY = sum(FY2)) ## # A tibble: 1 × 3 ## SumSquareMY SumSquareMF SumSquareFY ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12053. 6301. 5752. But we also have one more bit of information we need to consider - what we call degrees of freedom. Let’s say we want to put those sum squares into a nice table. We could also add in a row talking about the degrees of freedom. There are degrees of freedom for: M - which is 1. There is only 1 bit of information in the whole group mean F - which is 3. There are only 3 means in the conditions. Y - which is 30. We have 30 data points to observe. If you were to calculate the degrees of freedom (dof) for MY, MF and FY - you can run the same calculation: MY (Ydof - Mdof = 30 - 1 = 29) MF (Fdof - Mdof = 3 - 1 = 2) FY (Ydof - Fdof = 30 - 3 = 27) We can calculate something called mean squares which is: Mean Squares = Sum of Squares / Degrees of Freedom So: MS-MY = (12053.2/ 29) = 415.6276 MS-MF = (6301.4/2) = 3150.7 MS-FY = (5751.8/27) = 213.0296 And this is pretty much all we can do with the data. # I&#39;m using the package &#39;knitr&#39; to display this. knitr::kable(MFY |&gt; summarise(SumSquareMY = sum(MY2), SumSquareMF = sum(MF2), SumSquareFY = sum(FY2), MeanSquareMY = sum(MY2)/29, MeanSquareMF = sum(MF2)/2, MeanSquareFY = sum(FY2)/27)) SumSquareMY SumSquareMF SumSquareFY MeanSquareMY MeanSquareMF MeanSquareFY 12053.2 6301.4 5751.8 415.6276 3150.7 213.0296 Earlier we said variance was important because it took into account the size of the dataset. Mean Squares are very similar to the variance calculation, because they’re a measure of deviation in relation to the size of the dataset. 10.1.2.7 The Importance of Mean Squares If the condition (radio) has no effect on the data, then the variation we would see between the farms would be similar to the variation we saw within any given farm. It would be like Scenario 2, where the mean of the farm was no more use to us than the mean of the overall population. If that were the case: the Condition Mean Square (FMS) / Error Mean Square (EMS) = 1 Let’s look again at that last table. knitr::kable(MFY |&gt; summarise(SumSquareMY = sum(MY2), SumSquareMF = sum(MF2), SumSquareFY = sum(FY2), MeanSquareMY = sum(MY2)/29, MeanSquareMF = sum(MF2)/2, MeanSquareFY = sum(FY2)/27)) SumSquareMY SumSquareMF SumSquareFY MeanSquareMY MeanSquareMF MeanSquareFY 12053.2 6301.4 5751.8 415.6276 3150.7 213.0296 And now we will take the Condition Mean Square and Error Mean Squares from that table: FMS = 3150.7 ENS = 213.0296 And we’ll call this the F Ratio. 3150.7/213.0296 ## [1] 14.78996 10.1.2.8 F Ratios and F Distributions Because the mean squares are standardised by the size of the dataset, we can mathematically calculate the range and likelihood of any F-Ratio. This is called the F Distribution. You can look up F distributions for an alpha level of 0.05. Click here to go directly there. We are interested in the ratio between 2 and 27 degrees of freedom, which gives us a critical F ratio of 3.3541, which our F Ratio (14.79) is much bigger than. In fact, if we were to plot a curve of the F Distribution between 2 and 27 degrees of freedom, less than 0.001% of the total area would be more than 14.79. 10.1.2.9 In Conclusion . . . The probability of getting an F-Ratio as large as 14.8 (or larger), if the null hypothes (x has no effect on y) is true, is less than 0.001 Or: F (between 2 and 27 degrees of freedom) = 14.79, and P &lt; 0.001 CONGRATULATIONS! You just ran an ANOVA completely by hand. 10.1.2.10 Prove It Of course, we don’t run ANOVAs by hand. We don’t use all these steps to run an ANOVA. Instead we go back to the RadioTidy data, which if you View(RadioTidy), you will note doesn’t have any of our squares or sum squares or mean squares calculated. And we ask R to run an ANOVA: ANOVA &lt;- aov(DustYield ~ Radio, data = RadioTidy) summary(ANOVA) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Radio 2 6301 3151 14.79 4.6e-05 *** ## Residuals 27 5752 213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And that command summary(ANOVA) gives us a table exactly like the one we calculated by ourselves. When we say that radio has a significant effect on the dust yield of unicorns in an ANOVA (F2,27=14.79, P&lt;0.001) - you now know exactly what those numbers refer to. And now that you’ve run it by yourself, what would you recommend to the unicorn farmers of Scotland? "],["choosestats.html", "Chapter 11 How to choose your stats 11.1 Where to start 11.2 My response is a categorical variable 11.3 My response is a numerical variable 11.4 My response is an ordinal variable 11.5 I got all of them (generalised linear model)", " Chapter 11 How to choose your stats This chapter is potentially controversial. I deliberately haven’t discussed statistics in great detail in this book, not least because I feel I re-learn stats every few years. This chapter is not setting out to teach you statistics. There are many statistical assumptions and issues that I breeze past in this chapter. In this chapter I want to give you a quick reference to running statistical analyses in R, and, importantly, interpreting those results. A much, much better book is Andy Field, Jeremy Miles, and Zoe Field’s brilliant Discovering Statistics Using R book. You can skip this chapter if: You are comfortable deciding which statistical test to use So its happened. You’ve collected your data, you’ve installed R, and now you’re ready to run some analyses! But what on earth do you do next? How do you choose and interpret the right statistic? To be brutally honest, only repeated practice and engaging with statistics can help build this skill, and the more you learn, the more uncertain you may feel! But what this chapter can do is to give you a bit of a cheat sheet and show you the ‘greatest hits’ of statistical testing in R. I will also use the APA 7th Edition Numbers and Statistics guide to report the statistical tests, and show you how to extract that information from the output, but you should also look up the style guide of your desired report (e.g. APA 7th Edition). For this chapter I will use in-built R datasets and fake datasets to demonstrate these tests. I will also use the tidyverse package throughout (which offers a few more datasets), so you will want to have that loaded. I will note here all the packages that are used in this chapter, and highlight them at their specific tests. Remember, you can always install a package with install.packages(“package_name”) library(tidyverse) library(vcd) library(broom) library(pgirmess) library(clinfun) 11.1 Where to start To work your way through this chapter, you need to identify your: Response variable (the y axis) Explanatory variable (the x axis) and then work your way down this handy list: I have a . . . Categorical response variable And I want to compare it to a historical/known condition or I have only one sample group And I want to compare it with repeated measures of the same response in one sample group And I want to compare it across two groups And I want to predict it with a numerical response Numerical response variable And I want to compare it to a historical/known condition or I have only one sample group And I want to compare it with repeated measures of the same response in one sample group And I want to compare it across two groups And I want to predict it with a numerical response Ordinal response variable And I want to compare it to a historical/known condition or I have only one sample group And I want to compare it with repeated measures of the same response in one sample group And I want to compare it across two groups And I want to predict it with a numerical response Uh, I got all of them I think 11.2 My response is a categorical variable 11.2.1 Categorical Response, Historical/Known Comparisons 11.2.1.1 Proportion of responses different from a known/expected result (1 Sample Proportion Test) Our Data Let’s say we have 20 German Shepherd dogs, and we know that the prevalence of hip dysplasia, a very painful condition, is usually 18% in this breed. We want to know if these particular dogs (perhaps they are from a new genetic line) are showing less hip dysplasia than we would expect. We have recorded whether or not these dogs are showing signs of dysplasia as a ‘yes/no’ categorical variable, and 3 dogs are showing signs of hip dysplasia. We can run a 1 proportion test like so: binom.test(x = 3, n = 20, p = 0.18, alternative = &quot;two.sided&quot;) ## ## Exact binomial test ## ## data: 3 and 20 ## number of successes = 3, number of trials = 20, p-value = 1 ## alternative hypothesis: true probability of success is not equal to 0.18 ## 95 percent confidence interval: ## 0.03207094 0.37892683 ## sample estimates: ## probability of success ## 0.15 Note that we don’t even need any real ‘data’ to run this test - its simply drawn from statistical probabilities. We would write this up as: In this study, 15% of the dogs presented with signs of hip dysplacia, which is not significantly different from the historical prevalence of 18% in a binomial test (95% CI [3%-38%], p &gt; .9). What if we had more dogs? We’ve continued running this study, mainly because we’re worried that only having 20 dogs is not accounting for enough of the natural variation we see in the world. This time we have 50 dogs from this genetic line and we’ve seen 4 of them develop dysplasia. As we have more than 30 responses, we can use a different version of the test. prop.test(x = 4, n = 50, p = 0.18, alternative = &quot;two.sided&quot;) ## ## 1-sample proportions test with continuity correction ## ## data: 4 out of 50, null probability 0.18 ## X-squared = 2.7439, df = 1, p-value = 0.09763 ## alternative hypothesis: true p is not equal to 0.18 ## 95 percent confidence interval: ## 0.0259402 0.2011004 ## sample estimates: ## p ## 0.08 This time we have a little more information that we can report with: In this study, 8% of the dogs presented with hip dysplasia, which is not significantly different from the historical prevalence of 18% in a one sample proportion test (95% CI[3%-20%], X2 = 2.74, p = .098) 11.2.2 Categorical Response, Categorical Explanatory 11.2.2.1 Categorical Response (Repeated Measures) Categorical Explanatory, McNemar’s Test Our Data We have 40 German Shepherd dogs, some of whom already present with dysplasia, a painful condition. We’re concerned that after a season of work more dogs will develop dysplasia, and want to see if this is true. For this dataset we will create a contingency table: work &lt;- matrix(c(2, 5, 38, 35), ncol=2, byrow=TRUE, dimnames = list(c(&quot;Dysplasia&quot;, &quot;No Dysplasia&quot;), c(&quot;Before Work Season&quot;, &quot;After Work Season&quot;))) We are going to run a specific test which accounts for the fact we have the same dogs in both conditions (before and after work): mcnemar.test(work) ## ## McNemar&#39;s Chi-squared test with continuity correction ## ## data: work ## McNemar&#39;s chi-squared = 23.814, df = 1, p-value = 1.061e-06 After the work season significantly more dogs developed dysplasia in McNemar’s test (X2[1, n = 40] = 23.81, p = &lt;.001) 11.2.2.2 Proportion of responses compared across groups (Fishers/Chi2 test) 11.2.2.2.1 Fisher’s Exact Test (Small Sample Sizes / Summarised Data) Our Data German Shepherd Dogs suffer from hip dysplasia, a very painful condition. We know that more inbred dogs are more susceptible, and so we have followed 20 highly inbred dogs and 20 less inbred dogs and observed whether they developed the disease or not. As the prevalence is low (18% historically), we would not expect more than 3.6 dogs to develop dysplasia in either category (if it was left up to chance). For this reason, we should use Fisher’s Exact Test, which is more robust for smaller sample sizes. Of the inbred dogs, we’ve seen 4 cases of dysplasia. Of the less inbred dogs, we’ve seen 2 cases of dysplasia. Like before, we can test this without having any actual data in R: fisher.test(x = c(4,16), y = c(2,18)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: c(4, 16) and c(2, 18) ## p-value = 1 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.02564066 Inf ## sample estimates: ## odds ratio ## Inf We do need to calculate one more thing to add to our report,the odds ratio: (4+18) / (16 + 2) ## [1] 1.222222 And this is reported like so: A Fisher’s Exact Test found no significant difference in the frequency of dysplasia occurring in highly inbred dogs versus less inbred dogs (OR = 1.2, 95% CI[0.02-Inf], p &gt; .9) 11.2.2.2.2 Chi2 Test (Larger Sample Sizes) Our Data This time we have observed more German Shepherd Dogs, a highly inbred line which we think will be more prone to hip dysplasia, and a less inbred line which we think will be less prone to hip dysplasia. For this dataset we will create a contingency table: gsd &lt;- matrix(c(16, 12, 84, 86), ncol=2, byrow=TRUE, dimnames = list(c(&quot;Dysplasia&quot;, &quot;No Dysplasia&quot;), c(&quot;Inbred&quot;, &quot;Less Inbred&quot;))) With a Chi2 test we often also want to calculate an effect size, and there’s a handy package (vcd) which we can use to calculate Cramer’s V. We’ll calculate it here too. To interpret V, a small effect size is &lt;0.10, a medium effect size &lt; 0.30, and a large effect size is &lt;0.50. chisq.test(gsd) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: gsd ## X-squared = 0.30714, df = 1, p-value = 0.5794 #Odds Ratio (16 + 86) / (84 + 12) ## [1] 1.0625 library(vcd) assocstats(gsd) ## X^2 df P(&gt; X^2) ## Likelihood Ratio 0.57672 1 0.44760 ## Pearson 0.57481 1 0.44835 ## ## Phi-Coefficient : 0.054 ## Contingency Coeff.: 0.054 ## Cramer&#39;s V : 0.054 And report like so: A Chi2 test found no significant different in the frequency of dysplasia occurring in highly inbred dogs versus less inbred dos (OR 1.06, X2[1, n = 198] = 0.307, V = .05, p = .579) 11.2.3 Categorical Response, Numerical Explanatory (Logistic Regression) Logistic regression is a bit of a ‘catch-all’ phase, certainly in this chapter. It’s also having its own day in the sun and is now often called a form of machine learning. I have mixed feelings about this that we can’t get into right now. If you’re reading this chapter from top-to-bottom, this subheading will feel like a huge jump in difficulty. It is. We are beginning to step into the land of generalised linear models which are powerful statistical tools. If you’re very new to statistics and this is where your path has led you, you should really seek out advice from others. This is getting into the fun, meaty parts of statistical theory. But I did say I’d walk you through it . . . Our Data We have 21 German Shepherd Dogs. We took a measure of inflammation from their bodies, and then later recorded whether or not they were showing signs of hip dysplasia. We want to see if inflammation (a numerical score) can be used to predict whether the dogs will end up with hip dysplasia (a categorical variable, 0 = no signs, 1 = hip dysplasia). We will need to generate this data in R: dysp &lt;- tibble(dysplasia = c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), inflammation =c(0.91, 0.79, 1.40, 0.71, 1.01, 0.77, 0.85, 0.42, 1.02, 0.31, 0.05, 1.17, 0.04, 0.36, 0.12, 0.02, 0.05, 0.42, 0.92, 0.72, 1.05)) To run a binary logistic regression (predicting yes vs no from a numerical variable) we can use the glm command. We do need to take a few extra steps. For reporting we’ll also make use of the broom package. logit &lt;- glm(dysplasia ~ inflammation, data = dysp, family = &quot;binomial&quot;) summary(logit) ## ## Call: ## glm(formula = dysplasia ~ inflammation, family = &quot;binomial&quot;, ## data = dysp) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5715 -0.5727 -0.3094 1.0397 1.4914 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.190 1.484 -2.150 0.0316 * ## inflammation 3.488 1.740 2.004 0.0450 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 26.734 on 20 degrees of freedom ## Residual deviance: 20.534 on 19 degrees of freedom ## AIC: 24.534 ## ## Number of Fisher Scoring iterations: 5 logLik(logit) ## &#39;log Lik.&#39; -10.26719 (df=2) exp(cbind(OddsRatio = coef(logit), confint(logit))) ## OddsRatio 2.5 % 97.5 % ## (Intercept) 0.04118154 0.0008355444 0.4146414 ## inflammation 32.71638289 1.9151146706 2735.6121035 And we could report this like so: For each unit increase in inflammation, the odds of developing hip dysplasia increases by a factor of 32.7 in a binary logistic regression (Table 11.1) library(broom) tidy(logit) |&gt; mutate(OddsRatio = exp(coef(logit))) |&gt; select(term, estimate, std.error, statistic, OddsRatio, p.value ) |&gt; kable(caption = &quot;Coefficient table for variables in binary logistic regression predicting hip dysplasia\\nfrom inflammation markers in 21 German Shepard Dogs&quot;) Table: 11.1: Coefficient table for variables in binary logistic regression predicting hip dysplasia from inflammation markers in 21 German Shepard Dogs term estimate std.error statistic OddsRatio p.value (Intercept) -3.189765 1.48362 -2.149988 0.0411815 0.0315561 inflammation 3.487876 1.74028 2.004204 32.7163829 0.0450482 If you want to dig more deeply into logistic regressions, or model more predictor variables, I think this link is good. Remember to talk to your nearest friendly statistician. 11.3 My response is a numerical variable 11.3.1 Numerical Response, Historical/Known Comparisons (One Sample t-Test) Our Data The dataset trees gives us the height in feet of 31 black cherry trees. We want to know if these trees differ from a historical average height of 80ft. The t test command in R is simplistic, and so we do need to calculate the mean of height (mu) in addition to the t test to report properly. t.test(trees$Height, mu = 80) ## ## One Sample t-test ## ## data: trees$Height ## t = -3.4952, df = 30, p-value = 0.001496 ## alternative hypothesis: true mean is not equal to 80 ## 95 percent confidence interval: ## 73.6628 78.3372 ## sample estimates: ## mean of x ## 76 trees |&gt; summarise(mean= mean(Height), sd = sd(Height)) ## mean sd ## 1 76 6.371813 We would report this like so: The mean height (76ft +/- 6.37) for black cherry trees was smaller than the historical average of 80ft in a one-sample t-test (t(30) = -3.50, p = .001) 11.3.2 Numerical Response (Repeated Measures), Categorical Explanatory (Paired t-Test) Our Data In this example we have 10 housecats who we put on a special weight-loss diet. We weigh them before the diet and after, and we want to see if they have lost any weight. We can do this with a paired t-test. diet &lt;- tibble (before = c(5.04, 4.63, 4.04, 5.10, 5.43, 4.83, 3.45, 3.49, 5.02, 4.81), after = c( 4.78, 2.49, 4.46, 2.03, 5.13, 7.23, 3.50, 1.89, 3.30, 3.91)) diet |&gt; summarise(b_mean = mean(before), a_mean = mean(after), b_sd = sd(before), a_sd = sd(after)) ## # A tibble: 1 × 4 ## b_mean a_mean b_sd a_sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.58 3.87 0.689 1.62 t.test(diet$before, diet$after, paired = TRUE, alternative = &quot;two.sided&quot;) ## ## Paired t-test ## ## data: diet$before and diet$after ## t = 1.4615, df = 9, p-value = 0.1779 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -0.3900511 1.8140511 ## sample estimates: ## mean difference ## 0.712 After being on the weight-loss diet, cats weighed 3.9kg (+/- 1.62kg SD), compared to a mean weight of 4.6kg (+/- 0.69kg SD) before going on the diet, but this difference of 0.71kg (95% CI [-0.39, 1.81]) was not statistically significant in a paired t-test (t(9) = 1.46, p = .178) 11.3.3 Numerical Response, Categorical Explanatory (ANOVA/GLM) Traditionally, depending on how many explanatory variables you have and whether your design is balanced or unbalanced, you would select a different type of test, leading up to a general linear model as the most flexible form. One explanatory factor = 1 way ANOVA Two explanatory factors and balanced design = 2-way ANOVA Three or more explanatory factor or unbalanced design = general linear model (glm) I like using the Grafen and Hails approach to statistics, and considering all ANOVAs to be a glm. I am fundamentally lazy, and don’t like to remember three separate things when one model will do it all for me. Real statisicians may cry at this. Our data Let’s use the chickwts data to explore this. chickwts describes a simple experiment with six different feedtypes and the weights of the chicks fed on them. We are looking for the feedtype which provides the heaviest chick on average. As an ANOVA The basic steps of running a one-way ANOVA test is to create the model, inspect the model, check the model assumptions with a residual plot, and run pairwise comparisons to define the difference between the levels. Its also very difficult to interpret this without a plot so we’ll add on of those to our interpretation. chk_a &lt;- aov(weight ~ feed, data = chickwts) plot(chk_a, 1) summary(chk_a) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## feed 5 231129 46226 15.37 5.94e-10 *** ## Residuals 65 195556 3009 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(chk_a) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = weight ~ feed, data = chickwts) ## ## $feed ## diff lwr upr p adj ## horsebean-casein -163.383333 -232.346876 -94.41979 0.0000000 ## linseed-casein -104.833333 -170.587491 -39.07918 0.0002100 ## meatmeal-casein -46.674242 -113.906207 20.55772 0.3324584 ## soybean-casein -77.154762 -140.517054 -13.79247 0.0083653 ## sunflower-casein 5.333333 -60.420825 71.08749 0.9998902 ## linseed-horsebean 58.550000 -10.413543 127.51354 0.1413329 ## meatmeal-horsebean 116.709091 46.335105 187.08308 0.0001062 ## soybean-horsebean 86.228571 19.541684 152.91546 0.0042167 ## sunflower-horsebean 168.716667 99.753124 237.68021 0.0000000 ## meatmeal-linseed 58.159091 -9.072873 125.39106 0.1276965 ## soybean-linseed 27.678571 -35.683721 91.04086 0.7932853 ## sunflower-linseed 110.166667 44.412509 175.92082 0.0000884 ## soybean-meatmeal -30.480519 -95.375109 34.41407 0.7391356 ## sunflower-meatmeal 52.007576 -15.224388 119.23954 0.2206962 ## sunflower-soybean 82.488095 19.125803 145.85039 0.0038845 We first inspect the plot to ensure the residuals are normally distributed. If we saw a pattern here we would know that the model is consistently underfitting or overfitting some observations, and that one of the ANOVA’s key assumptions had not been met (in this case there are many future options but you really should be talking to a statistician or friendly peer at this point). There was a significant effect of feedtype on chick weight in a one-way ANOVA (F(5,65) = 15.37, p &lt;.001, Figure 11.1). Post-hoc comparisons with Tukeys HSD found variety in performance, with sunflower feeds significantly outperformed soybean (82g, p = .003) linseed (110g, p &lt;.001), and horsebean (168g, p &lt;.001). Casein diets outperform horsebean (163g, p &lt;.001), linseed (105g, p &lt;.001), and soybean (77g, p = .008). Meatmeal outperforms horsebean (117g, p &lt;.001) and soybean outperformed horsebean (86g, p = .004). chickwts |&gt; ggplot(aes(x = reorder(feed, -weight), y = weight)) + geom_boxplot()+ theme_classic() + labs(y = &quot;Weight (g)&quot;, x = &quot;Feedtype&quot;) Figure: 11.1: Chick weight (g) per feed type, data from ‘chickwts’ As a linear model The lm argument fits linear models and can do ANOVAs and regressions. Its my preferred way of running this kind of analysis, partly because I think the R2 number (a measure of how good a ‘fit’ the model is) is a really useful way to help present the data. We can also use the broom package for extra reporting and to visualise effects. chk_l &lt;- lm(weight ~ feed, data = chickwts) summary(chk_l) ## ## Call: ## lm(formula = weight ~ feed, data = chickwts) ## ## Residuals: ## Min 1Q Median 3Q Max ## -123.909 -34.413 1.571 38.170 103.091 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 323.583 15.834 20.436 &lt; 2e-16 *** ## feedhorsebean -163.383 23.485 -6.957 2.07e-09 *** ## feedlinseed -104.833 22.393 -4.682 1.49e-05 *** ## feedmeatmeal -46.674 22.896 -2.039 0.045567 * ## feedsoybean -77.155 21.578 -3.576 0.000665 *** ## feedsunflower 5.333 22.393 0.238 0.812495 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 54.85 on 65 degrees of freedom ## Multiple R-squared: 0.5417, Adjusted R-squared: 0.5064 ## F-statistic: 15.36 on 5 and 65 DF, p-value: 5.936e-10 library(broom) chk_l |&gt; tidy() |&gt; ggplot(aes(x = reorder(term, - estimate), y = estimate)) + geom_point(aes(colour = term, shape = term)) + geom_errorbar(aes(x = term, ymin = estimate-std.error, ymax = estimate+std.error, colour = term)) + theme_classic() Feedtype had a significant effect in a linear model, explaining 51% of the variance observed in chick weight. (F(5,65) = 15.36, p &lt; .001). Meatmeal had a moderately lower weight gain than casein (t = -2.04 p = .046), soybean (t = -3.58, p = &lt; .001), linseed (t = -4.68, p &lt; .001), and horsebean (t = -6.96, p &lt;.001) all had much lower weight gains. There was no significant difference between the casein diet and the sunflower diet (t = 0.24, p = .812). 11.3.4 Numerical Response, Numerical Explanatory (Correlation &amp; Regression) Our data We will use the trusty cars dataset which describes the speed (mph) and stopping distance (ft) of cars in the 1920s to talk about two types of test, Pearson’s correlation and linear regression. As Pearson’s correlation As many, many, many folk say, correlation is not causation. In statistical terms, that means that correlation does not aim to predict y from x, only to see if there is an underlying structure to the variation. cor.test(x = cars$speed, y = cars$dist, alternative = &quot;two.sided&quot;, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: cars$speed and cars$dist ## t = 9.464, df = 48, p-value = 1.49e-12 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6816422 0.8862036 ## sample estimates: ## cor ## 0.8068949 Speed has a strong, positive association with the stopping distance of cars in a correlation (r = .81, 95%CI[.68, .89], t(48) = 9.46, p &lt;.001). As a regression In a linear model, we can be more definitive in our reporting. carm &lt;- lm(dist ~ speed, data = cars) summary(carm) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 Stopping distance increased by 3.9ft for every mph faster the car travelled at (R2 = 64%, F(1,48) = 89.6, p&lt;.001) 11.4 My response is an ordinal variable 11.4.1 Ordinal Response, Historical/Known Comparisons (One Sample Wilcoxan Test) Our data In this example we have student ratings of this textbook from 1 (very unsatisfied) to 5 (very satisfied). We’re hoping that at the very least, people are satisfied, and have ranked the book a 4. For this we can use the One Sample Wilcoxan Test, but we need to calculate an additional step, the median. ratings &lt;- tibble(satisfaction = c(1,2,1,1,3,2,1,5,1,2,5,1,2,3,3,1,3)) ratings |&gt; summarise(median = median(satisfaction)) ## # A tibble: 1 × 1 ## median ## &lt;dbl&gt; ## 1 2 wilcox.test(ratings$satisfaction, mu = 4) ## ## Wilcoxon signed rank test with continuity correction ## ## data: ratings$satisfaction ## V = 7, p-value = 0.0009212 ## alternative hypothesis: true location is not equal to 4 The textbook achieved a medium score of ‘2’, or ‘unsatisfied’ from students, which was significantly lower than the desired median score of ‘4’ or ‘satisfied’ in a one-sample Wilcoxan test (V = 7, p = &lt;.001). 11.4.2 Ordinal Response (Repeated Measures), Categorical Explanatory (One Sample Wilcoxan Signed-Rank Test) Our data In this example we have student ratings of this textbook from 1 (very unsatisfied) to 5 (very satisfied). We made some changes to the book after the first set of reviews and we’re hoping the same students will now score the book higher. For this we can use the Wilcoxan Signed Rank Test, but we need to calculate an additional step, the median. ratings &lt;- tibble(satisfaction1 = c(1,2,1,1,3,2,1,5,1,2,5,1,2,3,3,1,3), satisfaction2 = c(1,4,2,3,3,3,2,5,2,4,5,3,3,3,4,1,3)) ratings |&gt; summarise(satis1 = median(satisfaction1), satis2 = median(satisfaction2)) ## # A tibble: 1 × 2 ## satis1 satis2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2 3 wilcox.test(x = ratings$satisfaction1, ratings$satisfaction2, paired = TRUE, alternative = &quot;two.sided&quot;) ## ## Wilcoxon signed rank test with continuity correction ## ## data: ratings$satisfaction1 and ratings$satisfaction2 ## V = 0, p-value = 0.004565 ## alternative hypothesis: true location shift is not equal to 0 After revisions, the textbook received a median score of ‘3’ (undecided), which was significantly higher than the before revision median score of ‘2’ (unsatisfied) in a Wilcoxan Signed Rank Test (V = 0, p = .005). 11.4.3 Ordinal Response, Categorical Explanatory Our data In this example, we have measured student satisfaction with their R teaching on a 5-point scale from 1 (very unsatisfied) to 5 (very satisfied). We have three conditions, students who searched their own materials (self-taught), students who received this textbook (textbook), and students who received the textbook plus tutorials (textbook plus). We want to know which students were most satisfied. For this we can use a Kruskal-Wallis test. We can use two more functions, kruskalmc from library(pgirmess) and jonckheere.test from library(clinfun) to help us interpret the data. Jonkheere-Terpstra tests are technically a different type of test, but are useful in exploring how ordered differences occur, e.g. in this case we expect that each intervention will improve student satisfaction, and that textbook-plus will be better than textbook, which itself will be better than self-taught. library(clinfun) library(pgirmess) textbk &lt;- tibble(self_taught = c(1,2,1,1,3,2,1,5,1,2,5,1,2,3,3,1,3), textbook= c(1,4,2,3,3,3,2,5,2,4,5,3,3,3,4,1,3), textbook_plus = c( 3, 2, 3, 3, 3, 3, 2, 4, 4, 5, 3, 4, 4, 5, 4, 5, 3)) |&gt; pivot_longer(cols = c(self_taught:textbook_plus), names_to = &quot;condition&quot;, values_to = &quot;score&quot;) kruskal.test(score ~ condition, textbk) ## ## Kruskal-Wallis rank sum test ## ## data: score by condition ## Kruskal-Wallis chi-squared = 10.503, df = 2, p-value = 0.00524 kruskalmc(score ~ condition, textbk, cont = &quot;two-tailed&quot;) ## Multiple comparison test after Kruskal-Wallis, treatments vs control (two-tailed) ## alpha: 0.05 ## Comparisons ## obs.dif critical.dif stat.signif ## self_taught-textbook 9.705882 11.42896 FALSE ## self_taught-textbook_plus 15.882353 11.42896 TRUE jonckheere.test(textbk$score, as.numeric(as.factor(textbk$condition))) ## ## Jonckheere-Terpstra test ## ## data: ## JT = 616, p-value = 0.001609 ## alternative hypothesis: two.sided Student satisfaction increased significantly with intervention (H(2) = 10.5, p* = .005), with textbook-plus showing a significant difference (p&lt;.05) from self-taught, but no difference between self-taught and textbook (p&gt;.05). The directionality of effect was significant in a Jonckheere-Terpstra test (T = 616, p = .002, Figure 11.2) textbk |&gt; ggplot(aes(x = condition, y = score)) + geom_boxplot()+ theme_classic() + scale_y_continuous(breaks = seq(1,5,1), limits = c(1,5), labels = c(&quot;Very unsatisfied&quot;, &quot;Unsatisfied&quot;, &quot;Not sure&quot;, &quot;Satisfied&quot;, &quot;Very satisfied&quot;)) + scale_x_discrete(labels = c(&quot;Self-Taught&quot;, &quot;Textbook&quot;, &quot;Textbook plus lecture&quot;)) + labs(x = &quot;Teaching Method&quot;, y = &quot;Score&quot;) Figure: 11.2: Student satisfaction by teaching method 11.4.4 Ordinal Response, Numerical Explanatory (Spearman rank correlation) Our data When you have an ordinal response and either a numerical explanatory or an ordinal explanatory variable, a Spearman rank correlation is probably for you. A Spearman rank correlation is exactly the same as a Pearson correlation, but the data is ranked (or ordered) before the calculation is run. Spearman rank correlations are therefore often used for skewed data. Let’s say we have 15 students who have ranked their satisfaction with the course (from 1, “Very unsatisfied” to 5, “Very satisfied”) and their grades (E, “very poor”, to A “excellent”). We think students who are very satisfied will have higher grades. grades &lt;- tibble(satisfaction = c(1, 5, 2, 1, 1, 5, 4, 5, 4, 5, 5, 3, 4, 3, 1), grade = c(1, 4, 1,2, 1, 4, 5, 5, 5, 4, 3, 3, 2, 2,1 )) cor.test(grades$satisfaction, grades$grade,method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: grades$satisfaction and grades$grade ## S = 121.25, p-value = 0.0005493 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7834775 Student satisfaction was strongly positively associated with grade (rs = .78,p&lt;.001) 11.5 I got all of them (generalised linear model) If your data is looking extremely complicated, and you have multiple explanatory variables, you likely want a glm. At this point you really should consult a statistician and start thinking seriously about your model assumptions . . . "],["whatnext.html", "Chapter 12 What next 12.1 github 12.2 Experiment and build 12.3 The R Community", " Chapter 12 What next You can skip this chapter if: You have no desire to take R any further I learned R in 2006, as part of my undergraduate degree. For the next ten years I used it sporadically, mainly when my stats got complicated and I could no longer rely on other statistical softwares to help. Starting with the complicated stuff meant that I always had a lot of heartache. In 2016, I finally took the plunge and deleted those other softwares, meaning I had to use R (and my newly discovered R Studio) for everything. It was the best decision I made. If you want to get better at R, you need to use it. Use it for the little things as well as the big things. Nowadays, I find it easier to work in R than in the other statistical softwares. In this, short, chapter, I will talk about some of the ways I’ve found that have helped me take R further. 12.1 github I would encourage anyone who’s interested in developing their R or coding skills to get a github account. Github is a mechanism for sharing code and resources. If you get yourself a free github account you can create a repository where you can store project files. For example, here’s the repository for this book and here’s a repository for processing data in WhatsApp chats. With code freely available and accessible, you can improve your skills by seeing what others do, and what you might like to start doing. There are many tutorials about github, here are a few to get you started: How to use github GitGuides Hello World Toward Data Science’s Complete beginners guide And you will find many more on the internet. There are a few advantages to using github: Collaboration is easier! With ‘pull requests’, it can be clear what you and what your colleagues have contributed to a project. You can roll back mistakes easily - and share all your code quickly You can work on projects wherever you are You can pull git projects from any computer, which means you can work more easily. One thing to be wary of on github is the uploading of data. While you can upload your data to github to make your project sharing easier, you should always make sure you’re happy for that data to be public facing! For example, if you’re working on survey data, you can only upload that data if all your participants have agreed for the data to be publicly facing. Private repositories are a way around this - but they can cost a subscription to github. 12.2 Experiment and build Following on from getting a github is the biggest tip I can give you. Start to experiment with what you can do. Experimenting always means failing. I particularly like this YouTube video of someone trying to create an algorithm to make recipes because it faithfully shows the failing process - Sabrina: I taught an AI to make pasta You cannot expect to get better if you don’t make mistakes. Why don’t you take a scroll through my github to see all the abandoned and broken projects I have. 12.3 The R Community Finally, I have also found the R Community to be a great resource. While back in 2006 I thought it was an unfriendly place, I’ve really warmed to it. You can find great blogs and resources from RStudio including: Their awesome RStudio Conference which always has very cool presentaitons. But there are also resources like the: #Rstats on Twitter - where people post queries or tips or tricks or just funny memes. And there are also lots of @RLadies communities to support women to use R more confidently. This is part of the RLadies initiative. "],["whyr.html", "Chapter 13 Why use R 13.1 Why does open source matter? 13.2 So why R?", " Chapter 13 Why use R Why is this chapter at the end of the book? Well, partly I wanted to get you guys started with R as quickly as possible. Partly, I think the ‘why’ of R only starts to make sense when you learn about workflows. With this said, it is important to think about Why we should use open source software and open data science. There’s no learning outcomes in this chapter. Instead, its more of a thesis, or more accurately, a blog post. 13.1 Why does open source matter? If you’ve come to this book because I’m teaching you R in one of my courses, or as a PhD student of mine, then you have probably heard my thoughts on why science needs to become more transparent. ( If you haven’t, you can watch me talk about it here ). I take the view that science is a philosophy, it is a way of thinking about the world. We use science to help us answer questions about the natural world. The process of collecting data and analysing data is not free from sources of bias. This is why reporting methodologies is such an important tenet of scientific process. Small variations in methodology can account for persistent effects that become reported as ‘truth’. Mice, for example, are stressed by male handlers, which may have increased observed stress in many trials, as we rarely report the gender of the mouse handlers. Many medical studies focus only on male participants, resulting in treatments that have lower efficacy for women. Many medical treatments are calibrated with the wrong kind of statistical test, meaning that a true understanding of what being measured is often missing. And of course we have the replication crisis, a discovery that a majority of psychology results cannot be replicated in independent trials. I believe that psychology is the tip of the iceberg, and that we will see more and more fields endure a replication crisis. There is a brilliant New York Time Magazine article about Amy Cuddy which I encourage all my students to read. It weaves a story of academic ambition, the fundamental issues with reproducibility in science, and the important personal relationships in science. I really recommend you read it. The point of all this is that science is not an infallible process that arrives at an objective truth. It is a process that we need to discuss, a process we need to review, and a process we need to critique in order to better understand our world. Alongside this, there is a push in science to be more open and transparent about data. There is a difference between: 71 chicks were allocated into six groups, with each group given a different feed supplement. Their weight in grams after six weeks was compared in an ANOVA, and feed was found to significantly affect weight (F5,65 = 15.37, p &lt; 0.001) And specifiying exactly: summary(aov(formula = weight ~ feed, data = chickwts)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## feed 5 231129 46226 15.37 5.94e-10 *** ## Residuals 65 195556 3009 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There are whole organisations dedicated to providing resources on open data and open practices in science. I personally try to provide my data and my analyses for my papers. Its not always possible (sometimes the data is too sensitive to be shared, sometimes another member of a team is ideologically opposed, and that’s fair too), and I hope that if I make a mistake, it won’t be replicated throughout the scientific world. Mistakes can be very costly. It is nervewracking though. Sharing your practice, how you arrived at a conclusion, can feel like you are exposing yourself to be found out for the imposter you really are. I have an awful lot of privilege in my life that allows me to accept this risk. See Amy Cuddy. Science needs to become kinder, and safer for all people, for these ideas to take hold. 13.2 So why R? R is an open source software, meaning that absolutely anyone can download it, use it, and improve it. It also has a low barrier to entry in that there are lots of free resources to help you learn. A scientist in Scotland can directly collaborate with a scientist in Malawi, sharing code, sharing resources, and improving the scientific world. R, and the way that we teach R and use R, is not free of biases. There are other programs, other methods, but I use R because I think it is a more transparent and accountable way of doing research. And ultimately, that’s what I’m striving for. "],["answers.html", "Chapter 14 Answers 14.1 Creating Data Answers 14.2 Loading csv data answers 14.3 Loading excel data answers 14.4 ggplot answers 14.5 Proof of variance 14.6 Tidy Unicorns", " Chapter 14 Answers 14.1 Creating Data Answers Here are the answers to Creating Data 14.1.1 Changing mpg data Changing one variable is pretty easy to do in tidyverse: mpg2 &lt;- mpg |&gt; mutate(manufacturer = as.factor(manufacturer)) But what if we want to change all the character variables? We can still use mutate like this: mpg2 &lt;- mpg |&gt; mutate(manufacturer = as.factor(manufacturer), model = as.factor(model), trans = as.factor(trans), drv = as.factor(drv), fl = as.factor(fl), class = as.factor(class)) Or we can use a slightly different command to apply the as.factor function across selected variables, predictably called mutate_at() mpg2 &lt;- mpg |&gt; mutate_at(.vars = vars(c(manufacturer, model, trans, drv, fl, class)), .funs = as.factor) mutate_at becomes a very useful way of shortening your code, but can also be a little bit more difficult to remember. I very often have to look it up. But that’s fine - looking up code is good :) 14.2 Loading csv data answers Here’s the answer to how to change the name of a dataset from a csv file dat &lt;- read.csv(&quot;~/data/ch2_planets.csv&quot;) Here is a chunk of text to hide the next answer from you in case you’re doing these answers sequentially. Figure: 14.1: Using the Import Dataset wizard to load a xlsx file 14.3 Loading excel data answers In this section I asked you to load data from a different folder and have a short cut to stop us having to load readxl each time If the below code doesn’t make sense to you - reach out to me! dat &lt;- readxl::read_excel(&quot;data/ch2_planets.xlsx&quot;) 14.4 ggplot answers 14.4.1 Boxplot of highway miles per gallon versus class (mpg dataset) Here is one possible solution: mpg |&gt; ggplot(aes(x = class, y = hwy)) + geom_boxplot() A prettier solution might be: mpg |&gt; ggplot(aes(x = reorder(class, -hwy), y = hwy)) + geom_boxplot() + labs (x = &quot;Car class&quot;, y = &quot;Highway miles per gallon&quot;) + theme_classic() 14.4.2 Histogram of city miles per gallon faceted by transmission type (mpg dataset) A possible solution: mpg |&gt; ggplot(aes(x = cty)) + geom_histogram() + facet_wrap(facets = ~ trans) 14.5 Proof of variance If we take our coos dataset again and create a new variable taking each value away from the mean: distances &lt;- coos |&gt; mutate(distances = mean(weight)-weight) distances ## # A tibble: 12 × 3 ## heifer_id weight distances ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 211. -0.683 ## 2 2 200. 10.2 ## 3 3 220. -9.48 ## 4 4 201. 9.82 ## 5 5 222 -11.4 ## 6 6 209. 1.32 ## 7 7 196. 14.8 ## 8 8 220. -9.78 ## 9 9 225. -14.6 ## 10 10 219. -8.08 ## 11 11 194. 16.9 ## 12 12 210. 0.917 And then we add all the distances together . . . round(sum(distances$distances), digits = 1) ## [1] 0 14.6 Tidy Unicorns We can tidy our Unicorn dataset with the pivot_longer command: RadioTidy &lt;- Radio |&gt; pivot_longer(cols = c(NoRadio:RadioDiscussion), names_to = &quot;Radio&quot;, values_to = &quot;DustYield&quot;) "],["acks.html", "Chapter 15 Acknowledgements 15.1 Image Sources 15.2 Specific Acknowledgements 15.3 Beta Testers and Helpers 15.4 Contributors 15.5 The biggest thanks", " Chapter 15 Acknowledgements 15.1 Image Sources Rat sketch by Gordon Johnson from Pixabay, Creative Commons 0 license. Lightbulb icon by OpenClipart-Vectors from Pixabay, Creative Commons 0 license Books icon by OpenClipart-Vectors from Pixabay, Creative Commons 0 license I stole the wonderful Warnings and Errors infographic from Rex-analytics 15.2 Specific Acknowledgements Ian Handel fixed the CSS, and also got me to delete Minitab from my laptop in 2016 in favour of RStudio. Thanks Ian! 15.3 Beta Testers and Helpers Steph Smith @ Edinburgh Uni 15.4 Contributors Steve Thorn 15.5 The biggest thanks And most importantly, when I wrote this book in the very stressful summer of 2020, Fraser brought me many cups of tea, chocolate biscuits, and kept me from going crazy. Thanks, babe. Footnotes might contain something useful - or something silly. I guess you’ll need to click it to see! But remember, you can’t trust me implicitly - or anyone really↩︎ If you’re reading this as part of your coursework you might be panicking about plagiarism here, after all, we spend a lot of time telling you plagiarism is the worst thing you could ever do and that we’ll use software to detect it. Code is a bit different. We are always trying to find the most efficient way of doing something, and so ideally you would all write code that was identical. Sadly, humans naturally differ in the way the think about problems. My job would be a lot easier if everybody thought the same. If I have set you this book as reading, I can swear to you I will never put your code through a plagiarism checker. That would be very stupid.↩︎ ‘Default’ here means one you won’t need to install or load into the library↩︎ If its not working - are you sure you have spelled it with a capital View?↩︎ The Length of these variables is the number of rows in each one, which for this case is the same for each variable because this is a nice, tidy dataset↩︎ A factor is also called a categorical variable, or a grouping variable. If you’re not sure you know what a factor is, wiki is a good place to review↩︎ is.character(y) should give you an error message like Error: object 'y' is not found because y by itself does not exist in your environment. There’s a way around this by ‘attaching’ data to your environment, which is a bit old fashioned and can result in problems down the line with your workflow (because you won’t necessarily know if the person you’re working with has also attached the data), so I recommend against it. is.character(dat::y) should give you an error like Error in loadNamespace(name) : there is no package called 'dat'. Unsurprisingly, this is telling you that the :: sequence tells R to look inside a package for a thing called y, but that package doesn’t exist. Packages and data frames are different things.↩︎ I like nord so much its the colour scheme for this book↩︎ The hinges are actually calculated based on the boxplot.stats function which is a little more complicated than 95% of the data - but if you ever need to worry about this you have gone beyond what this book can teach you.↩︎ This section is nearly wholly stolen from David Robinson↩︎ or web search of your choice↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
